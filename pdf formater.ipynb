{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOmy3K99uChTK/uuzWXyk7r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravi-katta-dev/News/blob/main/pdf%20formater.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "6ntwO0RMxh1w",
        "outputId": "591cdaef-76ad-4311-8818-aa6fa88148d4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 1514 (ipython-input-804855316.py, line 1515)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-804855316.py\"\u001b[0;36m, line \u001b[0;32m1515\u001b[0m\n\u001b[0;31m    \"\"\"Run interactive mode with step-by-step guidance\"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 1514\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ðŸŽ¯ COMPLETE INTELLIGENT PDF SPLITTER - Full Featured Solution\n",
        "# =============================================================================\n",
        "# Author: Advanced PDF Processing System\n",
        "# Date: 2025-08-08 12:18:45 UTC\n",
        "# User: Ravi-katta-dev\n",
        "# Version: 4.0 - Complete Solution\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "import zipfile\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, List, Dict, Any\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import fitz  # PyMuPDF\n",
        "    from tqdm.auto import tqdm\n",
        "    from google.colab import files\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    print(\"âœ… All imports successful!\")\n",
        "    print(f\"ðŸ• Session started: 2025-08-08 12:18:45 UTC\")\n",
        "    print(f\"ðŸ‘¤ User: Ravi-katta-dev\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Import error: {e}\")\n",
        "    print(\"Please install required packages:\")\n",
        "    print(\"!pip install PyMuPDF tqdm ipywidgets\")\n",
        "    raise\n",
        "\n",
        "class CompletePDFSplitter:\n",
        "    \"\"\"Complete PDF Splitter with all advanced features\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.processed_files = []\n",
        "        self.debug_mode = True\n",
        "        self.session_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        print(f\"ðŸŽ¯ Complete PDF Splitter v4.0 initialized!\")\n",
        "        print(f\"ðŸ“± Session ID: {self.session_id}\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # ðŸ“ VERTICAL LINE DETECTION SYSTEM\n",
        "    # =============================================================================\n",
        "\n",
        "    def detect_vertical_lines(self, pdf_path: str, sample_pages: int = 10) -> Dict:\n",
        "        \"\"\"Detect vertical lines in PDF for exact split positioning\"\"\"\n",
        "\n",
        "        doc = fitz.open(pdf_path)\n",
        "\n",
        "        try:\n",
        "            total_pages = len(doc)\n",
        "            sample_pages = min(sample_pages, total_pages)\n",
        "\n",
        "            print(f\"ðŸ“ Analyzing {sample_pages} pages for vertical lines...\")\n",
        "\n",
        "            all_vertical_lines = []\n",
        "            page_analyses = []\n",
        "\n",
        "            for page_num in range(sample_pages):\n",
        "                page = doc[page_num]\n",
        "\n",
        "                # Multi-method line detection\n",
        "                lines_from_paths = self._detect_lines_from_paths(page)\n",
        "                lines_from_vectors = self._detect_lines_from_vectors(page)\n",
        "                lines_from_text = self._detect_lines_from_text(page)\n",
        "                lines_from_images = self._detect_lines_from_images(page)\n",
        "\n",
        "                # Combine all methods\n",
        "                page_lines = []\n",
        "                page_lines.extend(lines_from_paths)\n",
        "                page_lines.extend(lines_from_vectors)\n",
        "                page_lines.extend(lines_from_text)\n",
        "                page_lines.extend(lines_from_images)\n",
        "\n",
        "                # Remove duplicates and filter\n",
        "                page_lines = self._filter_and_clean_lines(page_lines, page.rect.width)\n",
        "\n",
        "                page_analysis = {\n",
        "                    'page_num': page_num + 1,\n",
        "                    'lines': page_lines,\n",
        "                    'page_width': page.rect.width,\n",
        "                    'page_height': page.rect.height,\n",
        "                    'methods_count': {\n",
        "                        'paths': len(lines_from_paths),\n",
        "                        'vectors': len(lines_from_vectors),\n",
        "                        'text': len(lines_from_text),\n",
        "                        'images': len(lines_from_images)\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                page_analyses.append(page_analysis)\n",
        "                all_vertical_lines.extend(page_lines)\n",
        "\n",
        "                if self.debug_mode:\n",
        "                    print(f\"ðŸ“„ Page {page_num + 1}: Found {len(page_lines)} vertical lines\")\n",
        "                    if page_lines:\n",
        "                        ratios = [line / page.rect.width for line in page_lines]\n",
        "                        print(f\"   â€¢ Positions: {[f'{r:.1%}' for r in ratios[:5]]}\")\n",
        "\n",
        "            # Analyze all detected lines\n",
        "            final_analysis = self._analyze_all_detected_lines(all_vertical_lines, page_analyses)\n",
        "\n",
        "            return final_analysis\n",
        "\n",
        "        finally:\n",
        "            doc.close()\n",
        "\n",
        "    def _detect_lines_from_paths(self, page) -> List[float]:\n",
        "        \"\"\"Method 1: Detect lines from PDF drawing paths\"\"\"\n",
        "        vertical_lines = []\n",
        "\n",
        "        try:\n",
        "            drawings = page.get_drawings()\n",
        "\n",
        "            for drawing in drawings:\n",
        "                items = drawing.get(\"items\", [])\n",
        "\n",
        "                for item in items:\n",
        "                    if item[0] == \"l\":  # Line command\n",
        "                        if len(item) >= 3:\n",
        "                            p1, p2 = item[1], item[2]\n",
        "\n",
        "                            # Check for vertical line\n",
        "                            if abs(p1.x - p2.x) < 3:  # Tolerance for vertical\n",
        "                                line_length = abs(p2.y - p1.y)\n",
        "                                if line_length > page.rect.height * 0.3:\n",
        "                                    vertical_lines.append(p1.x)\n",
        "\n",
        "                    elif item[0] == \"re\":  # Rectangle (thin vertical)\n",
        "                        if len(item) >= 2:\n",
        "                            rect = item[1]\n",
        "                            if rect.width <= 5 and rect.height > page.rect.height * 0.2:\n",
        "                                vertical_lines.append(rect.x0 + rect.width / 2)\n",
        "\n",
        "                    elif item[0] == \"c\":  # Curve (might be decorative line)\n",
        "                        if len(item) >= 4:\n",
        "                            p1, p2, p3, p4 = item[1], item[2], item[3], item[4]\n",
        "                            # Check if curve is essentially vertical\n",
        "                            x_variance = max(p1.x, p2.x, p3.x, p4.x) - min(p1.x, p2.x, p3.x, p4.x)\n",
        "                            if x_variance < 5:\n",
        "                                avg_x = (p1.x + p2.x + p3.x + p4.x) / 4\n",
        "                                vertical_lines.append(avg_x)\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.debug_mode:\n",
        "                print(f\"   âš ï¸ Path detection error: {e}\")\n",
        "\n",
        "        return vertical_lines\n",
        "\n",
        "    def _detect_lines_from_vectors(self, page) -> List[float]:\n",
        "        \"\"\"Method 2: Detect lines from SVG vector graphics\"\"\"\n",
        "        vertical_lines = []\n",
        "\n",
        "        try:\n",
        "            svg_text = page.get_svg_image()\n",
        "\n",
        "            # Enhanced SVG pattern matching\n",
        "            patterns = [\n",
        "                r'<line[^>]*x1=\"([^\"]*)\"[^>]*y1=\"([^\"]*)\"[^>]*x2=\"([^\"]*)\"[^>]*y2=\"([^\"]*)\"[^>]*/?>',\n",
        "                r'<rect[^>]*x=\"([^\"]*)\"[^>]*y=\"([^\"]*)\"[^>]*width=\"([^\"]*)\"[^>]*height=\"([^\"]*)\"[^>]*/?>',\n",
        "                r'<path[^>]*d=\"M\\s*([^,\\s]+)[,\\s]+([^,\\s]+)\\s*L\\s*([^,\\s]+)[,\\s]+([^,\\s]+)\"[^>]*/?>',\n",
        "            ]\n",
        "\n",
        "            for pattern in patterns:\n",
        "                matches = re.findall(pattern, svg_text)\n",
        "\n",
        "                for match in matches:\n",
        "                    try:\n",
        "                        if len(match) == 4:\n",
        "                            if 'line' in pattern or 'path' in pattern:\n",
        "                                x1, y1, x2, y2 = map(float, match)\n",
        "                                if abs(x1 - x2) < 3:  # Vertical line\n",
        "                                    line_length = abs(y2 - y1)\n",
        "                                    if line_length > page.rect.height * 0.2:\n",
        "                                        vertical_lines.append(x1)\n",
        "\n",
        "                            elif 'rect' in pattern:\n",
        "                                x, y, width, height = map(float, match)\n",
        "                                if width <= 5 and height > page.rect.height * 0.2:\n",
        "                                    vertical_lines.append(x + width / 2)\n",
        "\n",
        "                    except (ValueError, IndexError):\n",
        "                        continue\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.debug_mode:\n",
        "                print(f\"   âš ï¸ Vector detection error: {e}\")\n",
        "\n",
        "        return vertical_lines\n",
        "\n",
        "    def _detect_lines_from_text(self, page) -> List[float]:\n",
        "        \"\"\"Method 3: Detect lines from text characters\"\"\"\n",
        "        vertical_lines = []\n",
        "\n",
        "        try:\n",
        "            text_dict = page.get_text(\"dict\")\n",
        "\n",
        "            # Extended line characters\n",
        "            line_chars = ['|', 'â”ƒ', 'â”‹', 'â”‡', 'â”†', 'â”‚', 'â•‘', 'Ç€', 'â', 'Ç', 'ï¸±', 'ä¸¨', 'ï½œ']\n",
        "\n",
        "            # Collect all text spans with line characters\n",
        "            line_positions = {}\n",
        "\n",
        "            for block in text_dict.get(\"blocks\", []):\n",
        "                if \"lines\" in block:\n",
        "                    for line in block[\"lines\"]:\n",
        "                        for span in line.get(\"spans\", []):\n",
        "                            text = span.get(\"text\", \"\")\n",
        "                            bbox = span.get(\"bbox\", [0, 0, 0, 0])\n",
        "\n",
        "                            for char in line_chars:\n",
        "                                if char in text:\n",
        "                                    char_x = round((bbox[0] + bbox[2]) / 2)\n",
        "\n",
        "                                    if char_x not in line_positions:\n",
        "                                        line_positions[char_x] = 0\n",
        "                                    line_positions[char_x] += text.count(char)\n",
        "\n",
        "            # Filter positions with enough line characters\n",
        "            for x_pos, count in line_positions.items():\n",
        "                if count >= 3:  # At least 3 line characters\n",
        "                    vertical_lines.append(float(x_pos))\n",
        "\n",
        "            # Also detect repeated patterns that might indicate borders\n",
        "            full_text = page.get_text()\n",
        "\n",
        "            # Look for repeated sequences that might be borders\n",
        "            border_patterns = ['|', '-|', '|-', '||', 'â”‚', 'â”€â”€']\n",
        "\n",
        "            for pattern in border_patterns:\n",
        "                if full_text.count(pattern) > 5:  # Pattern appears multiple times\n",
        "                    # This suggests there might be a structured layout\n",
        "                    # Add common positions for such patterns\n",
        "                    page_width = page.rect.width\n",
        "                    common_ratios = [0.25, 0.33, 0.4, 0.5, 0.6, 0.67, 0.75]\n",
        "\n",
        "                    for ratio in common_ratios:\n",
        "                        if any(abs(line/page_width - ratio) < 0.05 for line in vertical_lines):\n",
        "                            # Only add if we already detected something near this ratio\n",
        "                            vertical_lines.append(page_width * ratio)\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.debug_mode:\n",
        "                print(f\"   âš ï¸ Text line detection error: {e}\")\n",
        "\n",
        "        return vertical_lines\n",
        "\n",
        "    def _detect_lines_from_images(self, page) -> List[float]:\n",
        "        \"\"\"Method 4: Detect lines from embedded images\"\"\"\n",
        "        vertical_lines = []\n",
        "\n",
        "        try:\n",
        "            # Get images on the page\n",
        "            image_list = page.get_images()\n",
        "\n",
        "            for img in image_list:\n",
        "                try:\n",
        "                    bbox = page.get_image_bbox(img)\n",
        "                    if bbox:\n",
        "                        # Check if image is thin and vertical (might be a line)\n",
        "                        if bbox.width <= 10 and bbox.height > page.rect.height * 0.2:\n",
        "                            vertical_lines.append(bbox.x0 + bbox.width / 2)\n",
        "\n",
        "                        # Check for images that might contain line graphics\n",
        "                        if bbox.width > 20 and bbox.height > page.rect.height * 0.5:\n",
        "                            # This might be a large image with embedded lines\n",
        "                            # Add common split positions within the image\n",
        "                            img_center = bbox.x0 + bbox.width / 2\n",
        "\n",
        "                            # Check if image spans a significant portion of the page\n",
        "                            if bbox.width > page.rect.width * 0.3:\n",
        "                                vertical_lines.append(img_center)\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.debug_mode:\n",
        "                print(f\"   âš ï¸ Image line detection error: {e}\")\n",
        "\n",
        "        return vertical_lines\n",
        "\n",
        "    def _filter_and_clean_lines(self, lines: List[float], page_width: float) -> List[float]:\n",
        "        \"\"\"Filter and clean detected lines\"\"\"\n",
        "        if not lines:\n",
        "            return []\n",
        "\n",
        "        # Remove duplicates with tolerance\n",
        "        cleaned_lines = []\n",
        "        lines.sort()\n",
        "\n",
        "        for line in lines:\n",
        "            # Check if this line is valid (within page bounds and reasonable position)\n",
        "            if 0.1 * page_width <= line <= 0.9 * page_width:\n",
        "                # Check if it's too close to an existing line\n",
        "                if not any(abs(line - existing) < page_width * 0.02 for existing in cleaned_lines):\n",
        "                    cleaned_lines.append(line)\n",
        "\n",
        "        return cleaned_lines\n",
        "\n",
        "    def _analyze_all_detected_lines(self, all_lines: List[float], page_analyses: List[Dict]) -> Dict:\n",
        "        \"\"\"Analyze all detected lines to find optimal split\"\"\"\n",
        "\n",
        "        if not all_lines or not page_analyses:\n",
        "            return self._get_fallback_analysis()\n",
        "\n",
        "        page_width = page_analyses[0]['page_width']\n",
        "\n",
        "        # Convert to ratios and filter\n",
        "        line_ratios = []\n",
        "        for line in all_lines:\n",
        "            ratio = line / page_width\n",
        "            if 0.15 <= ratio <= 0.85:  # Reasonable split range\n",
        "                line_ratios.append(ratio)\n",
        "\n",
        "        if not line_ratios:\n",
        "            return self._get_fallback_analysis()\n",
        "\n",
        "        # Advanced clustering with multiple methods\n",
        "        clusters = self._perform_advanced_clustering(line_ratios)\n",
        "\n",
        "        if not clusters:\n",
        "            return self._get_fallback_analysis()\n",
        "\n",
        "        # Select best cluster\n",
        "        best_cluster = self._select_best_cluster(clusters, page_analyses)\n",
        "\n",
        "        # Calculate final metrics\n",
        "        optimal_split = sum(best_cluster['ratios']) / len(best_cluster['ratios'])\n",
        "        confidence = self._calculate_confidence(best_cluster, clusters, page_analyses)\n",
        "        consistency = self._calculate_consistency(best_cluster)\n",
        "\n",
        "        return {\n",
        "            'optimal_split': optimal_split,\n",
        "            'confidence': confidence,\n",
        "            'line_consistency': consistency,\n",
        "            'detected_lines': all_lines,\n",
        "            'line_ratios': line_ratios,\n",
        "            'clusters': clusters,\n",
        "            'best_cluster': best_cluster,\n",
        "            'method': 'advanced_line_detection',\n",
        "            'pages_analyzed': len(page_analyses),\n",
        "            'detection_summary': {\n",
        "                'total_lines_found': len(all_lines),\n",
        "                'valid_ratios': len(line_ratios),\n",
        "                'clusters_found': len(clusters),\n",
        "                'best_cluster_size': len(best_cluster['ratios'])\n",
        "            },\n",
        "            'page_details': page_analyses\n",
        "        }\n",
        "\n",
        "    def _perform_advanced_clustering(self, ratios: List[float]) -> List[Dict]:\n",
        "        \"\"\"Perform advanced clustering of line positions\"\"\"\n",
        "        if not ratios:\n",
        "            return []\n",
        "\n",
        "        ratios_sorted = sorted(ratios)\n",
        "        clusters = []\n",
        "\n",
        "        # Dynamic tolerance based on data distribution\n",
        "        tolerance = max(0.02, (max(ratios) - min(ratios)) / 10)\n",
        "\n",
        "        for ratio in ratios_sorted:\n",
        "            added = False\n",
        "\n",
        "            for cluster in clusters:\n",
        "                cluster_center = sum(cluster['ratios']) / len(cluster['ratios'])\n",
        "                if abs(ratio - cluster_center) <= tolerance:\n",
        "                    cluster['ratios'].append(ratio)\n",
        "                    added = True\n",
        "                    break\n",
        "\n",
        "            if not added:\n",
        "                clusters.append({\n",
        "                    'ratios': [ratio],\n",
        "                    'center': ratio,\n",
        "                    'weight': 1\n",
        "                })\n",
        "\n",
        "        # Update cluster centers and weights\n",
        "        for cluster in clusters:\n",
        "            cluster['center'] = sum(cluster['ratios']) / len(cluster['ratios'])\n",
        "            cluster['weight'] = len(cluster['ratios'])\n",
        "            cluster['spread'] = max(cluster['ratios']) - min(cluster['ratios']) if len(cluster['ratios']) > 1 else 0\n",
        "\n",
        "        return clusters\n",
        "\n",
        "    def _select_best_cluster(self, clusters: List[Dict], page_analyses: List[Dict]) -> Dict:\n",
        "        \"\"\"Select the best cluster based on multiple criteria\"\"\"\n",
        "\n",
        "        def score_cluster(cluster):\n",
        "            # Weight (frequency)\n",
        "            weight_score = cluster['weight'] / len(page_analyses)\n",
        "\n",
        "            # Consistency (low spread)\n",
        "            spread_score = 1 - min(1, cluster['spread'] / 0.1)\n",
        "\n",
        "            # Position preference (avoid extreme edges)\n",
        "            center = cluster['center']\n",
        "            position_score = 1 - abs(center - 0.5) * 2  # Prefer center positions\n",
        "\n",
        "            # Combined score\n",
        "            return (weight_score * 0.5 + spread_score * 0.3 + position_score * 0.2)\n",
        "\n",
        "        return max(clusters, key=score_cluster)\n",
        "\n",
        "    def _calculate_confidence(self, best_cluster: Dict, all_clusters: List[Dict], page_analyses: List[Dict]) -> float:\n",
        "        \"\"\"Calculate confidence in the detection\"\"\"\n",
        "\n",
        "        # Frequency confidence\n",
        "        frequency_conf = min(1.0, best_cluster['weight'] / len(page_analyses))\n",
        "\n",
        "        # Consistency confidence\n",
        "        consistency_conf = 1 - min(1, best_cluster['spread'] / 0.05)\n",
        "\n",
        "        # Method diversity confidence\n",
        "        total_methods = sum(\n",
        "            sum(page['methods_count'].values())\n",
        "            for page in page_analyses\n",
        "        )\n",
        "        method_conf = min(1.0, total_methods / (len(page_analyses) * 2))\n",
        "\n",
        "        # Dominance confidence (how much better is this cluster than others)\n",
        "        if len(all_clusters) > 1:\n",
        "            other_weights = [c['weight'] for c in all_clusters if c != best_cluster]\n",
        "            dominance_conf = best_cluster['weight'] / (max(other_weights) + 1) if other_weights else 1.0\n",
        "            dominance_conf = min(1.0, dominance_conf)\n",
        "        else:\n",
        "            dominance_conf = 1.0\n",
        "\n",
        "        # Combined confidence\n",
        "        confidence = (frequency_conf * 0.3 + consistency_conf * 0.3 +\n",
        "                     method_conf * 0.2 + dominance_conf * 0.2)\n",
        "\n",
        "        return confidence\n",
        "\n",
        "    def _calculate_consistency(self, cluster: Dict) -> float:\n",
        "        \"\"\"Calculate consistency of the cluster\"\"\"\n",
        "        if len(cluster['ratios']) <= 1:\n",
        "            return 1.0\n",
        "\n",
        "        return 1 - min(1, cluster['spread'] / 0.1)\n",
        "\n",
        "    def _get_fallback_analysis(self) -> Dict:\n",
        "        \"\"\"Fallback analysis when line detection fails\"\"\"\n",
        "        return {\n",
        "            'optimal_split': 0.5,\n",
        "            'confidence': 0.1,\n",
        "            'line_consistency': 0.0,\n",
        "            'detected_lines': [],\n",
        "            'method': 'fallback',\n",
        "            'detection_summary': {\n",
        "                'total_lines_found': 0,\n",
        "                'valid_ratios': 0,\n",
        "                'clusters_found': 0,\n",
        "                'best_cluster_size': 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # =============================================================================\n",
        "    # ðŸ§  CONTENT ANALYSIS SYSTEM\n",
        "    # =============================================================================\n",
        "\n",
        "    def analyze_content_layout(self, pdf_path: str, sample_pages: int = 5) -> Dict:\n",
        "        \"\"\"Comprehensive content analysis\"\"\"\n",
        "\n",
        "        doc = fitz.open(pdf_path)\n",
        "\n",
        "        try:\n",
        "            sample_pages = min(sample_pages, len(doc))\n",
        "            print(f\"ðŸ§  Analyzing content layout from {sample_pages} pages...\")\n",
        "\n",
        "            content_analysis = {\n",
        "                'text_density_map': [],\n",
        "                'layout_patterns': [],\n",
        "                'font_analysis': [],\n",
        "                'image_positions': [],\n",
        "                'whitespace_analysis': []\n",
        "            }\n",
        "\n",
        "            for page_num in range(sample_pages):\n",
        "                page = doc[page_num]\n",
        "\n",
        "                # Analyze text density\n",
        "                density_analysis = self._analyze_text_density(page)\n",
        "                content_analysis['text_density_map'].append(density_analysis)\n",
        "\n",
        "                # Analyze layout patterns\n",
        "                layout_analysis = self._analyze_layout_patterns(page)\n",
        "                content_analysis['layout_patterns'].append(layout_analysis)\n",
        "\n",
        "                # Analyze fonts and formatting\n",
        "                font_analysis = self._analyze_fonts_and_formatting(page)\n",
        "                content_analysis['font_analysis'].append(font_analysis)\n",
        "\n",
        "                # Analyze images and graphics\n",
        "                image_analysis = self._analyze_images_and_graphics(page)\n",
        "                content_analysis['image_positions'].append(image_analysis)\n",
        "\n",
        "                # Analyze whitespace\n",
        "                whitespace_analysis = self._analyze_whitespace_distribution(page)\n",
        "                content_analysis['whitespace_analysis'].append(whitespace_analysis)\n",
        "\n",
        "            # Combine all analyses\n",
        "            final_content_analysis = self._combine_content_analyses(content_analysis)\n",
        "\n",
        "            return final_content_analysis\n",
        "\n",
        "        finally:\n",
        "            doc.close()\n",
        "\n",
        "    def _analyze_text_density(self, page) -> Dict:\n",
        "        \"\"\"Analyze text density across the page\"\"\"\n",
        "        rect = page.rect\n",
        "\n",
        "        # Create density grid\n",
        "        grid_cols, grid_rows = 20, 20\n",
        "        density_grid = [[0 for _ in range(grid_cols)] for _ in range(grid_rows)]\n",
        "\n",
        "        # Get text blocks\n",
        "        text_dict = page.get_text(\"dict\")\n",
        "\n",
        "        for block in text_dict.get(\"blocks\", []):\n",
        "            if \"lines\" in block:\n",
        "                bbox = block[\"bbox\"]\n",
        "                text_length = 0\n",
        "\n",
        "                for line in block[\"lines\"]:\n",
        "                    for span in line.get(\"spans\", []):\n",
        "                        text_length += len(span.get(\"text\", \"\").strip())\n",
        "\n",
        "                if text_length > 0:\n",
        "                    # Map to grid\n",
        "                    col = min(int((bbox[0] + bbox[2]) / 2 / rect.width * grid_cols), grid_cols - 1)\n",
        "                    row = min(int((bbox[1] + bbox[3]) / 2 / rect.height * grid_rows), grid_rows - 1)\n",
        "\n",
        "                    density_grid[row][col] += text_length\n",
        "\n",
        "        # Analyze vertical distribution\n",
        "        vertical_density = [sum(density_grid[row][col] for row in range(grid_rows)) for col in range(grid_cols)]\n",
        "\n",
        "        # Find optimal vertical split\n",
        "        min_density_col = 0\n",
        "        min_density = float('inf')\n",
        "\n",
        "        for col in range(int(grid_cols * 0.2), int(grid_cols * 0.8)):\n",
        "            if vertical_density[col] < min_density:\n",
        "                min_density = vertical_density[col]\n",
        "                min_density_col = col\n",
        "\n",
        "        optimal_split = (min_density_col + 0.5) / grid_cols\n",
        "\n",
        "        return {\n",
        "            'density_grid': density_grid,\n",
        "            'vertical_density': vertical_density,\n",
        "            'optimal_split': optimal_split,\n",
        "            'total_text': sum(vertical_density)\n",
        "        }\n",
        "\n",
        "    def _analyze_layout_patterns(self, page) -> Dict:\n",
        "        \"\"\"Analyze layout patterns and structure\"\"\"\n",
        "        text_dict = page.get_text(\"dict\")\n",
        "\n",
        "        patterns = {\n",
        "            'columns': 0,\n",
        "            'alignment_patterns': {'left': 0, 'center': 0, 'right': 0},\n",
        "            'spacing_patterns': [],\n",
        "            'block_sizes': []\n",
        "        }\n",
        "\n",
        "        block_positions = []\n",
        "\n",
        "        for block in text_dict.get(\"blocks\", []):\n",
        "            if \"lines\" in block:\n",
        "                bbox = block[\"bbox\"]\n",
        "                block_width = bbox[2] - bbox[0]\n",
        "                block_height = bbox[3] - bbox[1]\n",
        "\n",
        "                patterns['block_sizes'].append({\n",
        "                    'width': block_width,\n",
        "                    'height': block_height,\n",
        "                    'x': bbox[0],\n",
        "                    'y': bbox[1]\n",
        "                })\n",
        "\n",
        "                block_positions.append(bbox[0])  # Left edge\n",
        "\n",
        "                # Analyze alignment\n",
        "                page_width = page.rect.width\n",
        "                if bbox[0] < page_width * 0.1:\n",
        "                    patterns['alignment_patterns']['left'] += 1\n",
        "                elif bbox[2] > page_width * 0.9:\n",
        "                    patterns['alignment_patterns']['right'] += 1\n",
        "                else:\n",
        "                    patterns['alignment_patterns']['center'] += 1\n",
        "\n",
        "        # Detect column structure\n",
        "        if block_positions:\n",
        "            block_positions.sort()\n",
        "            gaps = []\n",
        "\n",
        "            for i in range(1, len(block_positions)):\n",
        "                gap = block_positions[i] - block_positions[i-1]\n",
        "                if gap > 20:  # Significant gap\n",
        "                    gaps.append(gap)\n",
        "\n",
        "            patterns['spacing_patterns'] = gaps\n",
        "            patterns['columns'] = len(set(round(pos / 50) * 50 for pos in block_positions))\n",
        "\n",
        "        return patterns\n",
        "\n",
        "    def _analyze_fonts_and_formatting(self, page) -> Dict:\n",
        "        \"\"\"Analyze fonts and text formatting\"\"\"\n",
        "        text_dict = page.get_text(\"dict\")\n",
        "\n",
        "        font_analysis = {\n",
        "            'font_sizes': {},\n",
        "            'font_families': {},\n",
        "            'text_styles': {'bold': 0, 'italic': 0, 'normal': 0},\n",
        "            'color_distribution': {}\n",
        "        }\n",
        "\n",
        "        for block in text_dict.get(\"blocks\", []):\n",
        "            if \"lines\" in block:\n",
        "                for line in block[\"lines\"]:\n",
        "                    for span in line.get(\"spans\", []):\n",
        "                        # Font size\n",
        "                        size = span.get(\"size\", 12)\n",
        "                        size_key = f\"{size:.1f}\"\n",
        "                        font_analysis['font_sizes'][size_key] = font_analysis['font_sizes'].get(size_key, 0) + 1\n",
        "\n",
        "                        # Font family\n",
        "                        font = span.get(\"font\", \"unknown\")\n",
        "                        font_analysis['font_families'][font] = font_analysis['font_families'].get(font, 0) + 1\n",
        "\n",
        "                        # Text style\n",
        "                        flags = span.get(\"flags\", 0)\n",
        "                        if flags & 2**4:  # Bold\n",
        "                            font_analysis['text_styles']['bold'] += 1\n",
        "                        elif flags & 2**1:  # Italic\n",
        "                            font_analysis['text_styles']['italic'] += 1\n",
        "                        else:\n",
        "                            font_analysis['text_styles']['normal'] += 1\n",
        "\n",
        "        return font_analysis\n",
        "\n",
        "    def _analyze_images_and_graphics(self, page) -> Dict:\n",
        "        \"\"\"Analyze images and graphics distribution\"\"\"\n",
        "        images = page.get_images()\n",
        "        drawings = page.get_drawings()\n",
        "\n",
        "        image_analysis = {\n",
        "            'image_count': len(images),\n",
        "            'drawing_count': len(drawings),\n",
        "            'image_positions': [],\n",
        "            'drawing_positions': []\n",
        "        }\n",
        "\n",
        "        # Analyze image positions\n",
        "        for img in images:\n",
        "            try:\n",
        "                bbox = page.get_image_bbox(img)\n",
        "                if bbox:\n",
        "                    image_analysis['image_positions'].append({\n",
        "                        'x': bbox.x0,\n",
        "                        'y': bbox.y0,\n",
        "                        'width': bbox.width,\n",
        "                        'height': bbox.height,\n",
        "                        'center_x': bbox.x0 + bbox.width / 2\n",
        "                    })\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Analyze drawing positions\n",
        "        for drawing in drawings:\n",
        "            try:\n",
        "                rect = drawing.get(\"rect\")\n",
        "                if rect:\n",
        "                    image_analysis['drawing_positions'].append({\n",
        "                        'x': rect.x0,\n",
        "                        'y': rect.y0,\n",
        "                        'width': rect.width,\n",
        "                        'height': rect.height,\n",
        "                        'center_x': rect.x0 + rect.width / 2\n",
        "                    })\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        return image_analysis\n",
        "\n",
        "    def _analyze_whitespace_distribution(self, page) -> Dict:\n",
        "        \"\"\"Analyze whitespace distribution\"\"\"\n",
        "        text_dict = page.get_text(\"dict\")\n",
        "        page_width = page.rect.width\n",
        "        page_height = page.rect.height\n",
        "\n",
        "        # Create occupancy map\n",
        "        occupied_areas = []\n",
        "\n",
        "        for block in text_dict.get(\"blocks\", []):\n",
        "            if \"lines\" in block:\n",
        "                bbox = block[\"bbox\"]\n",
        "                occupied_areas.append(bbox)\n",
        "\n",
        "        # Add images\n",
        "        images = page.get_images()\n",
        "        for img in images:\n",
        "            try:\n",
        "                bbox = page.get_image_bbox(img)\n",
        "                if bbox:\n",
        "                    occupied_areas.append([bbox.x0, bbox.y0, bbox.x1, bbox.y1])\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Find largest vertical gaps\n",
        "        vertical_gaps = []\n",
        "\n",
        "        if occupied_areas:\n",
        "            # Sort by x-coordinate\n",
        "            x_positions = []\n",
        "            for area in occupied_areas:\n",
        "                x_positions.extend([area[0], area[2]])\n",
        "\n",
        "            x_positions.sort()\n",
        "\n",
        "            for i in range(1, len(x_positions)):\n",
        "                gap = x_positions[i] - x_positions[i-1]\n",
        "                if gap > page_width * 0.02:  # Significant gap\n",
        "                    gap_center = (x_positions[i] + x_positions[i-1]) / 2\n",
        "                    vertical_gaps.append({\n",
        "                        'position': gap_center,\n",
        "                        'size': gap,\n",
        "                        'ratio': gap_center / page_width\n",
        "                    })\n",
        "\n",
        "        return {\n",
        "            'vertical_gaps': vertical_gaps,\n",
        "            'largest_gap': max(vertical_gaps, key=lambda g: g['size']) if vertical_gaps else None\n",
        "        }\n",
        "\n",
        "    def _combine_content_analyses(self, content_analysis: Dict) -> Dict:\n",
        "        \"\"\"Combine all content analyses\"\"\"\n",
        "\n",
        "        # Average the results across pages\n",
        "        combined = {\n",
        "            'recommended_split': 0.5,\n",
        "            'confidence': 0.1,\n",
        "            'method': 'content_analysis',\n",
        "            'evidence': []\n",
        "        }\n",
        "\n",
        "        split_suggestions = []\n",
        "        confidences = []\n",
        "\n",
        "        # From text density\n",
        "        for density in content_analysis['text_density_map']:\n",
        "            split_suggestions.append(density['optimal_split'])\n",
        "            confidences.append(0.3)  # Base confidence for density analysis\n",
        "\n",
        "        # From whitespace analysis\n",
        "        for whitespace in content_analysis['whitespace_analysis']:\n",
        "            if whitespace['largest_gap']:\n",
        "                gap = whitespace['largest_gap']\n",
        "                if 0.2 <= gap['ratio'] <= 0.8:  # Reasonable range\n",
        "                    split_suggestions.append(gap['ratio'])\n",
        "                    confidences.append(0.7)  # Higher confidence for clear gaps\n",
        "\n",
        "        # Calculate weighted average\n",
        "        if split_suggestions and confidences:\n",
        "            total_weight = sum(confidences)\n",
        "            weighted_split = sum(s * c for s, c in zip(split_suggestions, confidences)) / total_weight\n",
        "            avg_confidence = sum(confidences) / len(confidences)\n",
        "\n",
        "            combined['recommended_split'] = weighted_split\n",
        "            combined['confidence'] = avg_confidence\n",
        "\n",
        "        return combined\n",
        "\n",
        "    # =============================================================================\n",
        "    # ðŸŽ¯ MULTI-METHOD DETECTION SYSTEM\n",
        "    # =============================================================================\n",
        "\n",
        "    def detect_optimal_split_multi_method(self, pdf_path: str) -> Dict:\n",
        "        \"\"\"Use multiple methods to detect optimal split ratio\"\"\"\n",
        "\n",
        "        print(\"ðŸŽ¯ Running comprehensive multi-method analysis...\")\n",
        "\n",
        "        # Method 1: Line Detection\n",
        "        print(\"ðŸ“ Method 1: Vertical line detection...\")\n",
        "        line_analysis = self.detect_vertical_lines(pdf_path)\n",
        "\n",
        "        # Method 2: Content Analysis\n",
        "        print(\"ðŸ§  Method 2: Content layout analysis...\")\n",
        "        content_analysis = self.analyze_content_layout(pdf_path)\n",
        "\n",
        "        # Method 3: Visual Pattern Recognition\n",
        "        print(\"ðŸ‘ï¸ Method 3: Visual pattern recognition...\")\n",
        "        visual_analysis = self._analyze_visual_patterns(pdf_path)\n",
        "\n",
        "        # Method 4: Document Structure Analysis\n",
        "        print(\"ðŸ“‹ Method 4: Document structure analysis...\")\n",
        "        structure_analysis = self._analyze_document_structure(pdf_path)\n",
        "\n",
        "        # Combine all methods\n",
        "        print(\"ðŸ”„ Combining all analysis methods...\")\n",
        "        final_analysis = self._combine_all_methods(\n",
        "            line_analysis, content_analysis, visual_analysis, structure_analysis\n",
        "        )\n",
        "\n",
        "        return final_analysis\n",
        "\n",
        "    def _analyze_visual_patterns(self, pdf_path: str) -> Dict:\n",
        "        \"\"\"Analyze visual patterns in the document\"\"\"\n",
        "\n",
        "        doc = fitz.open(pdf_path)\n",
        "\n",
        "        try:\n",
        "            # Sample pages for visual analysis\n",
        "            sample_pages = min(3, len(doc))\n",
        "            visual_patterns = []\n",
        "\n",
        "            for page_num in range(sample_pages):\n",
        "                page = doc[page_num]\n",
        "\n",
        "                # Convert page to image for visual analysis\n",
        "                mat = fitz.Matrix(1.0, 1.0)  # No scaling\n",
        "                pix = page.get_pixmap(matrix=mat)\n",
        "\n",
        "                # Analyze pixel patterns (simplified)\n",
        "                page_analysis = {\n",
        "                    'page_width': page.rect.width,\n",
        "                    'page_height': page.rect.height,\n",
        "                    'visual_split_suggestions': []\n",
        "                }\n",
        "\n",
        "                # Look for clear vertical divisions in content\n",
        "                # This is a simplified approach - in practice, you might use image processing\n",
        "                text_blocks = page.get_text(\"dict\").get(\"blocks\", [])\n",
        "\n",
        "                if text_blocks:\n",
        "                    x_positions = []\n",
        "                    for block in text_blocks:\n",
        "                        if \"bbox\" in block:\n",
        "                            bbox = block[\"bbox\"]\n",
        "                            x_positions.extend([bbox[0], bbox[2]])\n",
        "\n",
        "                    if x_positions:\n",
        "                        x_positions.sort()\n",
        "\n",
        "                        # Find gaps\n",
        "                        for i in range(1, len(x_positions)):\n",
        "                            gap = x_positions[i] - x_positions[i-1]\n",
        "                            if gap > page.rect.width * 0.05:  # Significant gap\n",
        "                                gap_center = (x_positions[i] + x_positions[i-1]) / 2\n",
        "                                gap_ratio = gap_center / page.rect.width\n",
        "\n",
        "                                if 0.2 <= gap_ratio <= 0.8:\n",
        "                                    page_analysis['visual_split_suggestions'].append(gap_ratio)\n",
        "\n",
        "                visual_patterns.append(page_analysis)\n",
        "\n",
        "            # Combine visual analysis\n",
        "            all_suggestions = []\n",
        "            for pattern in visual_patterns:\n",
        "                all_suggestions.extend(pattern['visual_split_suggestions'])\n",
        "\n",
        "            if all_suggestions:\n",
        "                optimal_split = sum(all_suggestions) / len(all_suggestions)\n",
        "                confidence = min(1.0, len(all_suggestions) / 3)  # More suggestions = higher confidence\n",
        "            else:\n",
        "                optimal_split = 0.5\n",
        "                confidence = 0.1\n",
        "\n",
        "            return {\n",
        "                'optimal_split': optimal_split,\n",
        "                'confidence': confidence,\n",
        "                'method': 'visual_pattern_analysis',\n",
        "                'suggestions': all_suggestions\n",
        "            }\n",
        "\n",
        "        finally:\n",
        "            doc.close()\n",
        "\n",
        "    def _analyze_document_structure(self, pdf_path: str) -> Dict:\n",
        "        \"\"\"Analyze document structure and metadata\"\"\"\n",
        "\n",
        "        doc = fitz.open(pdf_path)\n",
        "\n",
        "        try:\n",
        "            structure_analysis = {\n",
        "                'page_count': len(doc),\n",
        "                'page_dimensions': [],\n",
        "                'text_statistics': {},\n",
        "                'structure_hints': []\n",
        "            }\n",
        "\n",
        "            # Analyze first few pages for structure\n",
        "            sample_pages = min(5, len(doc))\n",
        "            total_text = \"\"\n",
        "\n",
        "            for page_num in range(sample_pages):\n",
        "                page = doc[page_num]\n",
        "\n",
        "                structure_analysis['page_dimensions'].append({\n",
        "                    'width': page.rect.width,\n",
        "                    'height': page.rect.height,\n",
        "                    'ratio': page.rect.width / page.rect.height\n",
        "                })\n",
        "\n",
        "                page_text = page.get_text()\n",
        "                total_text += page_text\n",
        "\n",
        "            # Analyze text for structural hints\n",
        "            structure_hints = []\n",
        "\n",
        "            # Check for exam/question patterns\n",
        "            exam_keywords = ['question', 'answer', 'select', 'choose', 'option', 'practice', 'test', 'exam']\n",
        "            for keyword in exam_keywords:\n",
        "                if keyword.lower() in total_text.lower():\n",
        "                    structure_hints.append(f\"exam_paper_{keyword}\")\n",
        "\n",
        "            # Check for telegram/social media references\n",
        "            social_keywords = ['telegram', 'join', 'click here', 'open', 'subscribe']\n",
        "            for keyword in social_keywords:\n",
        "                if keyword.lower() in total_text.lower():\n",
        "                    structure_hints.append(f\"social_media_{keyword}\")\n",
        "\n",
        "            # Check for two-column indicators\n",
        "            column_indicators = ['column', 'left', 'right', 'side']\n",
        "            for indicator in column_indicators:\n",
        "                if indicator.lower() in total_text.lower():\n",
        "                    structure_hints.append(f\"column_layout_{indicator}\")\n",
        "\n",
        "            structure_analysis['structure_hints'] = structure_hints\n",
        "\n",
        "            # Determine likely split based on structure\n",
        "            if any('exam_paper' in hint for hint in structure_hints):\n",
        "                if any('social_media' in hint for hint in structure_hints):\n",
        "                    # Exam paper with social media reference - likely split needed\n",
        "                    suggested_split = 0.6  # Questions on left (60%), social media on right (40%)\n",
        "                    confidence = 0.8\n",
        "                else:\n",
        "                    # Pure exam paper - might not need split or different ratio\n",
        "                    suggested_split = 0.5\n",
        "                    confidence = 0.4\n",
        "            else:\n",
        "                # Unknown structure\n",
        "                suggested_split = 0.5\n",
        "                confidence = 0.2\n",
        "\n",
        "            return {\n",
        "                'optimal_split': suggested_split,\n",
        "                'confidence': confidence,\n",
        "                'method': 'document_structure_analysis',\n",
        "                'structure_hints': structure_hints,\n",
        "                'analysis': structure_analysis\n",
        "            }\n",
        "\n",
        "        finally:\n",
        "            doc.close()\n",
        "\n",
        "    def _combine_all_methods(self, line_analysis: Dict, content_analysis: Dict,\n",
        "                           visual_analysis: Dict, structure_analysis: Dict) -> Dict:\n",
        "        \"\"\"Combine results from all analysis methods\"\"\"\n",
        "\n",
        "        methods = [\n",
        "            ('line_detection', line_analysis),\n",
        "            ('content_analysis', content_analysis),\n",
        "            ('visual_analysis', visual_analysis),\n",
        "            ('structure_analysis', structure_analysis)\n",
        "        ]\n",
        "\n",
        "        # Weight methods by their confidence\n",
        "        weighted_splits = []\n",
        "        weighted_confidences = []\n",
        "        method_details = {}\n",
        "\n",
        "        for method_name, analysis in methods:\n",
        "            split = analysis.get('optimal_split', 0.5)\n",
        "            confidence = analysis.get('confidence', 0.1)\n",
        "\n",
        "            weighted_splits.append(split * confidence)\n",
        "            weighted_confidences.append(confidence)\n",
        "\n",
        "            method_details[method_name] = {\n",
        "                'split': split,\n",
        "                'confidence': confidence,\n",
        "                'details': analysis\n",
        "            }\n",
        "\n",
        "        # Calculate final weighted average\n",
        "        total_weight = sum(weighted_confidences)\n",
        "\n",
        "        if total_weight > 0:\n",
        "            final_split = sum(weighted_splits) / total_weight\n",
        "            final_confidence = sum(weighted_confidences) / len(weighted_confidences)\n",
        "        else:\n",
        "            final_split = 0.5\n",
        "            final_confidence = 0.1\n",
        "\n",
        "        # Adjust confidence based on method agreement\n",
        "        splits_only = [analysis.get('optimal_split', 0.5) for _, analysis in methods]\n",
        "        split_variance = np.var(splits_only) if len(splits_only) > 1 else 0\n",
        "        agreement_factor = max(0.1, 1 - split_variance * 5)  # Higher variance = lower agreement\n",
        "\n",
        "        final_confidence *= agreement_factor\n",
        "\n",
        "        return {\n",
        "            'optimal_split': final_split,\n",
        "            'confidence': final_confidence,\n",
        "            'agreement_factor': agreement_factor,\n",
        "            'method': 'multi_method_combined',\n",
        "            'individual_methods': method_details,\n",
        "            'summary': {\n",
        "                'line_detection_confidence': line_analysis.get('confidence', 0),\n",
        "                'content_analysis_confidence': content_analysis.get('confidence', 0),\n",
        "                'visual_analysis_confidence': visual_analysis.get('confidence', 0),\n",
        "                'structure_analysis_confidence': structure_analysis.get('confidence', 0),\n",
        "                'methods_agreement': agreement_factor,\n",
        "                'recommended_split': final_split\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # =============================================================================\n",
        "    # âœ‚ï¸ PDF SPLITTING ENGINE\n",
        "    # =============================================================================\n",
        "\n",
        "    def split_pdf_with_analysis(self, input_path: str, output_path: str,\n",
        "                               method: str = \"multi_method\") -> bool:\n",
        "        \"\"\"Split PDF using comprehensive analysis\"\"\"\n",
        "\n",
        "        input_doc = None\n",
        "        output_doc = None\n",
        "\n",
        "        try:\n",
        "            # Step 1: Comprehensive Analysis\n",
        "            print(\"ðŸ” Step 1: Comprehensive PDF Analysis\")\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "            if method == \"multi_method\":\n",
        "                analysis = self.detect_optimal_split_multi_method(input_path)\n",
        "            elif method == \"line_detection\":\n",
        "                analysis = self.detect_vertical_lines(input_path)\n",
        "            elif method == \"content_analysis\":\n",
        "                analysis = self.analyze_content_layout(input_path)\n",
        "            else:\n",
        "                # Fallback to multi-method\n",
        "                analysis = self.detect_optimal_split_multi_method(input_path)\n",
        "\n",
        "            optimal_ratio = analysis.get('optimal_split', 0.5)\n",
        "            confidence = analysis.get('confidence', 0.1)\n",
        "\n",
        "            print(f\"\\nðŸ“Š Analysis Results:\")\n",
        "            print(f\"   â€¢ Detection method: {analysis.get('method', 'unknown')}\")\n",
        "            print(f\"   â€¢ Optimal split ratio: {optimal_ratio:.1%}\")\n",
        "            print(f\"   â€¢ Detection confidence: {confidence:.1%}\")\n",
        "\n",
        "            if 'summary' in analysis:\n",
        "                summary = analysis['summary']\n",
        "                print(f\"   â€¢ Line detection confidence: {summary.get('line_detection_confidence', 0):.1%}\")\n",
        "                print(f\"   â€¢ Content analysis confidence: {summary.get('content_analysis_confidence', 0):.1%}\")\n",
        "                print(f\"   â€¢ Methods agreement: {summary.get('methods_agreement', 0):.1%}\")\n",
        "\n",
        "            # Validate the detected ratio\n",
        "            if optimal_ratio < 0.15 or optimal_ratio > 0.85:\n",
        "                print(f\"âš ï¸ Warning: Detected ratio {optimal_ratio:.1%} seems extreme. Using safer ratio.\")\n",
        "                optimal_ratio = max(0.2, min(0.8, optimal_ratio))\n",
        "\n",
        "            if confidence < 0.3:\n",
        "                print(f\"âš ï¸ Warning: Low confidence ({confidence:.1%}). Results may not be optimal.\")\n",
        "\n",
        "            # Step 2: Apply Split\n",
        "            print(f\"\\nâœ‚ï¸ Step 2: Applying Split at {optimal_ratio:.1%}\")\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "            input_doc = fitz.open(input_path)\n",
        "            output_doc = fitz.open()\n",
        "\n",
        "            page_count = len(input_doc)\n",
        "            success_count = 0\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            with tqdm(total=page_count, desc=\"ðŸ“„ Processing\", unit=\"page\") as pbar:\n",
        "                for page_num in range(page_count):\n",
        "                    try:\n",
        "                        page = input_doc[page_num]\n",
        "                        rect = page.rect\n",
        "\n",
        "                        # Calculate split position\n",
        "                        split_pos = rect.width * optimal_ratio\n",
        "\n",
        "                        # Create left half\n",
        "                        left_clip = fitz.Rect(0, 0, split_pos, rect.height)\n",
        "                        left_page = output_doc.new_page(width=split_pos, height=rect.height)\n",
        "                        left_page.show_pdf_page(fitz.Rect(0, 0, split_pos, rect.height),\n",
        "                                               input_doc, page_num, clip=left_clip)\n",
        "\n",
        "                        # Create right half\n",
        "                        right_clip = fitz.Rect(split_pos, 0, rect.width, rect.height)\n",
        "                        right_page = output_doc.new_page(width=rect.width - split_pos, height=rect.height)\n",
        "                        right_page.show_pdf_page(fitz.Rect(0, 0, rect.width - split_pos, rect.height),\n",
        "                                                input_doc, page_num, clip=right_clip)\n",
        "\n",
        "                        success_count += 1\n",
        "                        pbar.set_postfix({\n",
        "                            'Split': f\"{optimal_ratio:.1%}\",\n",
        "                            'Success': f\"{success_count}/{page_count}\"\n",
        "                        })\n",
        "                        pbar.update(1)\n",
        "\n",
        "                        # Memory management\n",
        "                        if (page_num + 1) % 10 == 0:\n",
        "                            gc.collect()\n",
        "\n",
        "                    except Exception as page_error:\n",
        "                        print(f\"âš ï¸ Page {page_num + 1} error: {page_error}\")\n",
        "                        continue\n",
        "\n",
        "            # Step 3: Save Results\n",
        "            print(\"\\nðŸ’¾ Step 3: Saving Results\")\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "            if len(output_doc) == 0:\n",
        "                print(\"âŒ No pages were processed successfully\")\n",
        "                return False\n",
        "\n",
        "            try:\n",
        "                # Save with optimization\n",
        "                output_doc.save(output_path, garbage=4, deflate=True)\n",
        "\n",
        "                # Calculate performance metrics\n",
        "                elapsed_time = time.time() - start_time\n",
        "                output_size = os.path.getsize(output_path) / (1024 * 1024)\n",
        "                created_pages = len(output_doc)\n",
        "\n",
        "                print(f\"âœ… Processing Complete!\")\n",
        "                print(f\"\\nðŸ“Š Results Summary:\")\n",
        "                print(f\"   â€¢ Input file: {os.path.basename(input_path)}\")\n",
        "                print(f\"   â€¢ Output file: {os.path.basename(output_path)}\")\n",
        "                print(f\"   â€¢ Pages processed: {success_count}/{page_count}\")\n",
        "                print(f\"   â€¢ Pages created: {created_pages}\")\n",
        "                print(f\"   â€¢ Output size: {output_size:.2f} MB\")\n",
        "                print(f\"   â€¢ Processing time: {elapsed_time:.2f} seconds\")\n",
        "                print(f\"   â€¢ Speed: {page_count/elapsed_time:.1f} pages/second\")\n",
        "                print(f\"   â€¢ Split ratio used: {optimal_ratio:.1%}\")\n",
        "                print(f\"   â€¢ Detection confidence: {confidence:.1%}\")\n",
        "                print(f\"   â€¢ Success rate: {success_count/page_count:.1%}\")\n",
        "\n",
        "                return True\n",
        "\n",
        "            except Exception as save_error:\n",
        "                print(f\"âŒ Save error: {save_error}\")\n",
        "                # Try basic save as fallback\n",
        "                try:\n",
        "                    output_doc.save(output_path)\n",
        "                    print(\"âœ… Saved with basic options\")\n",
        "                    return True\n",
        "                except Exception as basic_error:\n",
        "                    print(f\"âŒ Basic save failed: {basic_error}\")\n",
        "                    return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error during processing: {e}\")\n",
        "            import traceback\n",
        "            if self.debug_mode:\n",
        "                print(f\"ðŸ” Debug traceback:\\n{traceback.format_exc()}\")\n",
        "            return False\n",
        "\n",
        "        finally:\n",
        "            # Cleanup\n",
        "            if input_doc:\n",
        "                try:\n",
        "                    input_doc.close()\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            if output_doc:\n",
        "                try:\n",
        "                    output_doc.close()\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            gc.collect()\n",
        "\n",
        "    # =============================================================================\n",
        "    # ðŸŽ›ï¸ USER INTERFACE AND CONTROL FUNCTIONS\n",
        "    # =============================================================================\n",
        "\n",
        "    def create_advanced_interface(self):\n",
        "        \"\"\"Create advanced user interface\"\"\"\n",
        "\n",
        "        print(\"ðŸŽ›ï¸ ADVANCED PDF SPLITTER INTERFACE\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Detection method selection\n",
        "        method_selector = widgets.Dropdown(\n",
        "            options=[\n",
        "                ('ðŸŽ¯ Multi-Method Analysis (Recommended)', 'multi_method'),\n",
        "                ('ðŸ“ Line Detection Only', 'line_detection'),\n",
        "                ('ðŸ§  Content Analysis Only', 'content_analysis'),\n",
        "                ('ðŸ‘ï¸ Visual Analysis Only', 'visual_analysis'),\n",
        "                ('ðŸ”§ Custom Ratio', 'custom')\n",
        "            ],\n",
        "            value='multi_method',\n",
        "            description='Analysis Method:',\n",
        "            style={'description_width': '150px'},\n",
        "            layout={'width': '400px'}\n",
        "        )\n",
        "\n",
        "        # Custom ratio slider (hidden by default)\n",
        "        custom_ratio_slider = widgets.FloatSlider(\n",
        "            value=0.5,\n",
        "            min=0.1,\n",
        "            max=0.9,\n",
        "            step=0.01,\n",
        "            description='Custom Split Ratio:',\n",
        "            style={'description_width': '150px'},\n",
        "            readout_format='.0%',\n",
        "            layout={'width': '400px', 'display': 'none'}\n",
        "        )\n",
        "\n",
        "        # Advanced options\n",
        "        debug_checkbox = widgets.Checkbox(\n",
        "            value=True,\n",
        "            description='Enable detailed analysis output',\n",
        "            style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        sample_pages_slider = widgets.IntSlider(\n",
        "            value=10,\n",
        "            min=1,\n",
        "            max=20,\n",
        "            description='Pages to analyze:',\n",
        "            style={'description_width': '150px'},\n",
        "            layout={'width': '400px'}\n",
        "        )\n",
        "\n",
        "        # Analysis results display\n",
        "        results_output = widgets.Output()\n",
        "\n",
        "        # Interactive elements\n",
        "        def on_method_change(change):\n",
        "            if change['new'] == 'custom':\n",
        "                custom_ratio_slider.layout.display = 'block'\n",
        "            else:\n",
        "                custom_ratio_slider.layout.display = 'none'\n",
        "\n",
        "        method_selector.observe(on_method_change, names='value')\n",
        "\n",
        "        # Layout\n",
        "        interface = widgets.VBox([\n",
        "            widgets.HTML(\"<h2>ðŸŽ¯ Advanced PDF Splitter Configuration</h2>\"),\n",
        "            widgets.HTML(\"<p>Select analysis method and configure options:</p>\"),\n",
        "            method_selector,\n",
        "            custom_ratio_slider,\n",
        "            debug_checkbox,\n",
        "            sample_pages_slider,\n",
        "            widgets.HTML(\"<hr>\"),\n",
        "            results_output\n",
        "        ])\n",
        "\n",
        "        display(interface)\n",
        "\n",
        "        return {\n",
        "            'method_selector': method_selector,\n",
        "            'custom_ratio_slider': custom_ratio_slider,\n",
        "            'debug_checkbox': debug_checkbox,\n",
        "            'sample_pages_slider': sample_pages_slider,\n",
        "            'results_output': results_output\n",
        "        }\n",
        "\n",
        "    def validate_pdf_file(self, pdf_path: str) -> Tuple[bool, str, Dict]:\n",
        "        \"\"\"Comprehensive PDF validation\"\"\"\n",
        "\n",
        "        try:\n",
        "            if not os.path.exists(pdf_path):\n",
        "                return False, \"âŒ File not found\", {}\n",
        "\n",
        "            if not pdf_path.lower().endswith('.pdf'):\n",
        "                return False, \"âŒ Not a PDF file\", {}\n",
        "\n",
        "            # Get file info\n",
        "            file_size = os.path.getsize(pdf_path) / (1024 * 1024)  # MB\n",
        "            file_modified = datetime.fromtimestamp(os.path.getmtime(pdf_path))\n",
        "\n",
        "            # Open and analyze PDF\n",
        "            doc = fitz.open(pdf_path)\n",
        "\n",
        "            try:\n",
        "                page_count = len(doc)\n",
        "\n",
        "                if page_count == 0:\n",
        "                    return False, \"âŒ PDF has no pages\", {}\n",
        "\n",
        "                if doc.needs_pass:\n",
        "                    return False, \"âŒ PDF is password protected\", {}\n",
        "\n",
        "                # Get detailed info\n",
        "                metadata = doc.metadata\n",
        "                first_page = doc[0]\n",
        "\n",
        "                pdf_info = {\n",
        "                    'file_size_mb': file_size,\n",
        "                    'page_count': page_count,\n",
        "                    'page_dimensions': {\n",
        "                        'width': first_page.rect.width,\n",
        "                        'height': first_page.rect.height,\n",
        "                        'aspect_ratio': first_page.rect.width / first_page.rect.height\n",
        "                    },\n",
        "                    'metadata': {\n",
        "                        'title': metadata.get('title', 'Unknown'),\n",
        "                        'author': metadata.get('author', 'Unknown'),\n",
        "                        'creator': metadata.get('creator', 'Unknown'),\n",
        "                        'producer': metadata.get('producer', 'Unknown')\n",
        "                    },\n",
        "                    'file_modified': file_modified.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                    'estimated_processing_time': page_count * 0.1  # seconds\n",
        "                }\n",
        "\n",
        "                # Check for potential issues\n",
        "                warnings = []\n",
        "\n",
        "                if file_size > 50:  # Large file\n",
        "                    warnings.append(f\"âš ï¸ Large file ({file_size:.1f} MB) - processing may take longer\")\n",
        "\n",
        "                if page_count > 100:\n",
        "                    warnings.append(f\"âš ï¸ Many pages ({page_count}) - consider processing in batches\")\n",
        "\n",
        "                if first_page.rect.width / first_page.rect.height < 1.2:\n",
        "                    warnings.append(\"âš ï¸ Pages seem narrow - vertical split may not be optimal\")\n",
        "\n",
        "                success_message = f\"âœ… Valid PDF: {page_count} pages, {file_size:.1f} MB\"\n",
        "                if warnings:\n",
        "                    success_message += f\"\\n   {'   '.join(warnings)}\"\n",
        "\n",
        "                return True, success_message, pdf_info\n",
        "\n",
        "            finally:\n",
        "                doc.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, f\"âŒ Error analyzing PDF: {str(e)}\", {}\n",
        "\n",
        "    def download_with_retry(self, file_path: str, max_retries: int = 3) -> bool:\n",
        "        \"\"\"Download file with retry logic and better error handling\"\"\"\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"âŒ File not found: {file_path}\")\n",
        "            return False\n",
        "\n",
        "        file_size = os.path.getsize(file_path) / (1024 * 1024)\n",
        "        print(f\"ðŸ“¥ Preparing download: {os.path.basename(file_path)} ({file_size:.2f} MB)\")\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                print(f\"   Attempt {attempt + 1}/{max_retries}...\")\n",
        "                files.download(file_path)\n",
        "                print(\"âœ… Download successful!\")\n",
        "                return True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   âš ï¸ Attempt {attempt + 1} failed: {str(e)[:100]}\")\n",
        "\n",
        "                if attempt < max_retries - 1:\n",
        "                    wait_time = (attempt + 1) * 2  # Progressive wait\n",
        "                    print(f\"   â³ Waiting {wait_time} seconds before retry...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"âŒ All download attempts failed\")\n",
        "                    print(f\"ðŸ’¡ File is still available locally: {file_path}\")\n",
        "                    return False\n",
        "\n",
        "        return False\n",
        "\n",
        "# =============================================================================\n",
        "# ðŸš€ MAIN EXECUTION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def test_comprehensive_analysis():\n",
        "    \"\"\"Test comprehensive analysis on uploaded PDF\"\"\"\n",
        "\n",
        "    print(\"ðŸ§ª COMPREHENSIVE PDF ANALYSIS TEST\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ðŸ• Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"ðŸ‘¤ User: Ravi-katta-dev\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Find PDF files\n",
        "    pdf_files = [f for f in os.listdir('.') if f.lower().endswith('.pdf') and not f.startswith(('SMART_', 'LINE_', 'PRECISION_'))]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"âŒ No PDF files found.\")\n",
        "        print(\"ðŸ’¡ Please upload a PDF file first.\")\n",
        "        return None\n",
        "\n",
        "    pdf_file = pdf_files[0]\n",
        "    print(f\"ðŸ“ Analyzing: {pdf_file}\")\n",
        "\n",
        "    # Create splitter instance\n",
        "    splitter = CompletePDFSplitter()\n",
        "\n",
        "    # Validate PDF\n",
        "    is_valid, message, pdf_info = splitter.validate_pdf_file(pdf_file)\n",
        "    print(f\"\\nðŸ“‹ PDF Validation:\")\n",
        "    print(message)\n",
        "\n",
        "    if not is_valid:\n",
        "        return None\n",
        "\n",
        "    if pdf_info:\n",
        "        print(f\"\\nðŸ“Š PDF Information:\")\n",
        "        print(f\"   â€¢ Pages: {pdf_info['page_count']}\")\n",
        "        print(f\"   â€¢ Size: {pdf_info['file_size_mb']:.2f} MB\")\n",
        "        print(f\"   â€¢ Dimensions: {pdf_info['page_dimensions']['width']:.0f} x {pdf_info['page_dimensions']['height']:.0f}\")\n",
        "        print(f\"   â€¢ Aspect ratio: {pdf_info['page_dimensions']['aspect_ratio']:.2f}\")\n",
        "        print(f\"   â€¢ Estimated processing time: {pdf_info['estimated_processing_time']:.1f} seconds\")\n",
        "\n",
        "    # Run comprehensive analysis\n",
        "    print(f\"\\nðŸ” Running Comprehensive Analysis...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    analysis_result = splitter.detect_optimal_split_multi_method(pdf_file)\n",
        "\n",
        "    # Display detailed results\n",
        "    print(f\"\\nðŸ“Š COMPREHENSIVE ANALYSIS RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ðŸŽ¯ Optimal Split Ratio: {analysis_result['optimal_split']:.1%}\")\n",
        "    print(f\"ðŸ”’ Overall Confidence: {analysis_result['confidence']:.1%}\")\n",
        "    print(f\"ðŸ¤ Methods Agreement: {analysis_result.get('agreement_factor', 0):.1%}\")\n",
        "\n",
        "    if 'summary' in analysis_result:\n",
        "        summary = analysis_result['summary']\n",
        "        print(f\"\\nðŸ“‹ Individual Method Confidences:\")\n",
        "        print(f\"   ðŸ“ Line Detection: {summary.get('line_detection_confidence', 0):.1%}\")\n",
        "        print(f\"   ðŸ§  Content Analysis: {summary.get('content_analysis_confidence', 0):.1%}\")\n",
        "        print(f\"   ðŸ‘ï¸ Visual Analysis: {summary.get('visual_analysis_confidence', 0):.1%}\")\n",
        "        print(f\"   ðŸ“‹ Structure Analysis: {summary.get('structure_analysis_confidence', 0):.1%}\")\n",
        "\n",
        "    # Recommendations\n",
        "    confidence = analysis_result['confidence']\n",
        "    split_ratio = analysis_result['optimal_split']\n",
        "\n",
        "    print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    if confidence > 0.8:\n",
        "        print(\"ðŸŸ¢ EXCELLENT: Very high confidence in detection\")\n",
        "        print(f\"   âœ… Proceed with split at {split_ratio:.1%}\")\n",
        "    elif confidence > 0.6:\n",
        "        print(\"ðŸŸ¡ GOOD: High confidence in detection\")\n",
        "        print(f\"   âœ… Recommended split at {split_ratio:.1%}\")\n",
        "    elif confidence > 0.4:\n",
        "        print(\"ðŸŸ  MODERATE: Moderate confidence\")\n",
        "        print(f\"   âš ï¸ Consider manual review of {split_ratio:.1%} split\")\n",
        "    else:\n",
        "        print(\"ðŸ”´ LOW: Low confidence in detection\")\n",
        "        print(f\"   âš ï¸ Manual inspection recommended\")\n",
        "        print(f\"   ðŸ’­ Suggested fallback: {split_ratio:.1%}\")\n",
        "\n",
        "    # Special cases\n",
        "    if split_ratio < 0.25 or split_ratio > 0.75:\n",
        "        print(f\"   âš ï¸ Unusual split ratio detected: {split_ratio:.1%}\")\n",
        "        print(\"   ðŸ’­ This might indicate special document layout\")\n",
        "\n",
        "    return analysis_result\n",
        "\n",
        "def run_intelligent_split():\n",
        "    \"\"\"Run intelligent PDF splitting with comprehensive analysis\"\"\"\n",
        "\n",
        "    print(\"ðŸŽ¯ INTELLIGENT PDF SPLITTER v4.0\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ðŸ§  Complete Multi-Method Analysis System\")\n",
        "    print(f\"ðŸ• Session: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"ðŸ‘¤ User: Ravi-katta-dev\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Find PDF files\n",
        "    pdf_files = [f for f in os.listdir('.') if f.lower().endswith('.pdf') and not f.startswith(('SMART_', 'LINE_', 'PRECISION_'))]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"âŒ No PDF files found.\")\n",
        "        print(\"ðŸ’¡ Please upload a PDF file first.\")\n",
        "        return\n",
        "\n",
        "    pdf_file = pdf_files[0]\n",
        "\n",
        "    # Create splitter\n",
        "    splitter = CompletePDFSplitter()\n",
        "\n",
        "    # Generate output filename\n",
        "    timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "    output_file = f\"INTELLIGENT_SPLIT_{Path(pdf_file).stem}_{timestamp}.pdf\"\n",
        "\n",
        "    print(f\"ðŸ“ Input: {pdf_file}\")\n",
        "    print(f\"ðŸ“ Output: {output_file}\")\n",
        "\n",
        "    # Process with comprehensive analysis\n",
        "    success = splitter.split_pdf_with_analysis(pdf_file, output_file, \"multi_method\")\n",
        "\n",
        "    if success:\n",
        "        print(f\"\\nðŸŽ‰ SUCCESS! PDF split completed.\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        # Attempt download\n",
        "        if splitter.download_with_retry(output_file):\n",
        "            print(\"âœ… File downloaded successfully!\")\n",
        "        else:\n",
        "            print(\"âš ï¸ Download failed, but file is ready locally\")\n",
        "            def run_interactive_mode():\n",
        "    \"\"\"Run interactive mode with step-by-step guidance\"\"\"\n",
        "\n",
        "    print(\"ðŸŽ® INTERACTIVE PDF SPLITTER MODE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ðŸ• Current Time: 2025-08-08 12:22:52 UTC\")\n",
        "    print(f\"ðŸ‘¤ Current User: Ravi-katta-dev\")\n",
        "    print(\"ðŸŽ¯ Interactive Step-by-Step Processing\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create splitter instance\n",
        "    splitter = CompletePDFSplitter()\n",
        "\n",
        "    # Step 1: Find and validate PDF\n",
        "    print(\"\\nðŸ“‹ STEP 1: PDF Discovery and Validation\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    pdf_files = [f for f in os.listdir('.') if f.lower().endswith('.pdf') and not f.startswith(('SMART_', 'LINE_', 'PRECISION_', 'INTELLIGENT_'))]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"âŒ No PDF files found in current directory.\")\n",
        "        print(\"\\nðŸ’¡ To upload a PDF:\")\n",
        "        print(\"   1. Click the ðŸ“ folder icon in the left sidebar\")\n",
        "        print(\"   2. Select 'Upload to session storage'\")\n",
        "        print(\"   3. Choose your PDF file\")\n",
        "        print(\"   4. Wait for upload to complete\")\n",
        "        print(\"   5. Re-run this function\")\n",
        "        return\n",
        "\n",
        "    print(f\"âœ… Found PDF files:\")\n",
        "    for i, pdf in enumerate(pdf_files, 1):\n",
        "        file_size = os.path.getsize(pdf) / (1024 * 1024)\n",
        "        print(f\"   {i}. {pdf} ({file_size:.2f} MB)\")\n",
        "\n",
        "    # Use first PDF\n",
        "    selected_pdf = pdf_files[0]\n",
        "    print(f\"\\nðŸŽ¯ Selected: {selected_pdf}\")\n",
        "\n",
        "    # Validate PDF\n",
        "    is_valid, validation_message, pdf_info = splitter.validate_pdf_file(selected_pdf)\n",
        "    print(f\"\\nðŸ“Š Validation Result:\")\n",
        "    print(validation_message)\n",
        "\n",
        "    if not is_valid:\n",
        "        print(\"âŒ Cannot proceed with invalid PDF.\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Quick Analysis Preview\n",
        "    print(f\"\\nðŸ” STEP 2: Quick Analysis Preview\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    print(\"Running quick line detection preview...\")\n",
        "    quick_analysis = splitter.detect_vertical_lines(selected_pdf, sample_pages=3)\n",
        "\n",
        "    quick_split = quick_analysis.get('optimal_split', 0.5)\n",
        "    quick_confidence = quick_analysis.get('confidence', 0.1)\n",
        "\n",
        "    print(f\"ðŸ“ Quick line detection result:\")\n",
        "    print(f\"   â€¢ Suggested split: {quick_split:.1%}\")\n",
        "    print(f\"   â€¢ Confidence: {quick_confidence:.1%}\")\n",
        "\n",
        "    if quick_confidence > 0.6:\n",
        "        print(\"âœ… High confidence - line detection working well!\")\n",
        "    elif quick_confidence > 0.3:\n",
        "        print(\"âš ï¸ Moderate confidence - will use comprehensive analysis\")\n",
        "    else:\n",
        "        print(\"âŒ Low confidence - will run full multi-method analysis\")\n",
        "\n",
        "    # Step 3: User Decision Point\n",
        "    print(f\"\\nðŸ¤” STEP 3: Processing Decision\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if quick_confidence > 0.7:\n",
        "        print(\"ðŸš€ RECOMMENDATION: Use quick line detection\")\n",
        "        print(f\"   Split ratio: {quick_split:.1%}\")\n",
        "        print(\"   This should work perfectly for your PDF!\")\n",
        "        recommended_method = \"line_detection\"\n",
        "    else:\n",
        "        print(\"ðŸ§  RECOMMENDATION: Use comprehensive multi-method analysis\")\n",
        "        print(\"   This will analyze your PDF using all available methods\")\n",
        "        recommended_method = \"multi_method\"\n",
        "\n",
        "    print(f\"\\nðŸ“ Available options:\")\n",
        "    print(\"   1. ðŸš€ Use recommended method (fastest)\")\n",
        "    print(\"   2. ðŸ§  Force comprehensive analysis (most accurate)\")\n",
        "    print(\"   3. ðŸ“ Line detection only\")\n",
        "    print(\"   4. ðŸŽ›ï¸ Custom ratio\")\n",
        "    print(\"   5. ðŸ§ª Test all methods (analysis only)\")\n",
        "\n",
        "    # Auto-select recommended method for demo\n",
        "    choice = \"1\"  # Simulating user choosing recommended method\n",
        "    print(f\"ðŸŽ¯ Auto-selecting option 1 (recommended method)\")\n",
        "\n",
        "    # Step 4: Execute Processing\n",
        "    print(f\"\\nâš™ï¸ STEP 4: Processing Execution\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Generate output filename\n",
        "    timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "\n",
        "    if choice == \"1\":\n",
        "        method = recommended_method\n",
        "        output_file = f\"SMART_SPLIT_{Path(selected_pdf).stem}_{timestamp}.pdf\"\n",
        "    elif choice == \"2\":\n",
        "        method = \"multi_method\"\n",
        "        output_file = f\"COMPREHENSIVE_SPLIT_{Path(selected_pdf).stem}_{timestamp}.pdf\"\n",
        "    elif choice == \"3\":\n",
        "        method = \"line_detection\"\n",
        "        output_file = f\"LINE_SPLIT_{Path(selected_pdf).stem}_{timestamp}.pdf\"\n",
        "    elif choice == \"4\":\n",
        "        # Custom ratio would be handled here\n",
        "        custom_ratio = 0.6  # Example\n",
        "        method = \"custom\"\n",
        "        output_file = f\"CUSTOM_SPLIT_{Path(selected_pdf).stem}_{timestamp}.pdf\"\n",
        "    else:\n",
        "        # Test mode\n",
        "        print(\"ðŸ§ª Running comprehensive test...\")\n",
        "        test_result = test_comprehensive_analysis()\n",
        "        print(\"âœ… Test completed! Check results above.\")\n",
        "        return\n",
        "\n",
        "    print(f\"ðŸ“ Output file: {output_file}\")\n",
        "    print(f\"ðŸ”§ Processing method: {method}\")\n",
        "\n",
        "    # Execute the split\n",
        "    print(f\"\\nðŸš€ Starting processing...\")\n",
        "    success = splitter.split_pdf_with_analysis(selected_pdf, output_file, method)\n",
        "\n",
        "    # Step 5: Results and Download\n",
        "    print(f\"\\nðŸ“¦ STEP 5: Results and Download\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if success:\n",
        "        print(\"ðŸŽ‰ SUCCESS! PDF processing completed.\")\n",
        "\n",
        "        # File statistics\n",
        "        if os.path.exists(output_file):\n",
        "            output_size = os.path.getsize(output_file) / (1024 * 1024)\n",
        "            input_size = os.path.getsize(selected_pdf) / (1024 * 1024)\n",
        "\n",
        "            print(f\"\\nðŸ“Š File Statistics:\")\n",
        "            print(f\"   â€¢ Input size: {input_size:.2f} MB\")\n",
        "            print(f\"   â€¢ Output size: {output_size:.2f} MB\")\n",
        "            print(f\"   â€¢ Size efficiency: {(1 - output_size/input_size):.1%} reduction\")\n",
        "\n",
        "        # Attempt download\n",
        "        print(f\"\\nðŸ“¥ Attempting download...\")\n",
        "        download_success = splitter.download_with_retry(output_file, max_retries=3)\n",
        "\n",
        "        if download_success:\n",
        "            print(\"âœ… Download completed successfully!\")\n",
        "            print(f\"ðŸ’¾ Your split PDF has been downloaded: {output_file}\")\n",
        "        else:\n",
        "            print(\"âš ï¸ Download failed, but processing was successful\")\n",
        "            print(f\"ðŸ’¡ File is available locally: {output_file}\")\n",
        "            print(\"ðŸ”„ You can try downloading manually or re-run the download\")\n",
        "    else:\n",
        "        print(\"âŒ Processing failed. Please check the error messages above.\")\n",
        "\n",
        "        # Troubleshooting suggestions\n",
        "        print(f\"\\nðŸ”§ Troubleshooting suggestions:\")\n",
        "        print(\"   1. Check if PDF is valid and not corrupted\")\n",
        "        print(\"   2. Try restarting the runtime (Runtime â†’ Restart Runtime)\")\n",
        "        print(\"   3. Re-upload the PDF file\")\n",
        "        print(\"   4. Try a different processing method\")\n",
        "\n",
        "    print(f\"\\nâœ… Interactive session completed!\")\n",
        "    print(f\"ðŸ• Session ended: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC\")\n",
        "\n",
        "def quick_split_now():\n",
        "    \"\"\"Quick split with minimal setup - just process and download\"\"\"\n",
        "\n",
        "    print(\"âš¡ QUICK SPLIT - FAST PROCESSING\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"ðŸ• Time: 2025-08-08 12:22:52 UTC\")\n",
        "    print(f\"ðŸ‘¤ User: Ravi-katta-dev\")\n",
        "    print(\"âš¡ Fast processing with smart defaults\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Find PDF\n",
        "    pdf_files = [f for f in os.listdir('.') if f.lower().endswith('.pdf') and not f.startswith(('SMART_', 'LINE_', 'PRECISION_', 'INTELLIGENT_', 'COMPREHENSIVE_', 'CUSTOM_'))]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"âŒ No PDF files found.\")\n",
        "        return\n",
        "\n",
        "    pdf_file = pdf_files[0]\n",
        "    print(f\"ðŸ“ Processing: {pdf_file}\")\n",
        "\n",
        "    # Quick validation\n",
        "    if not os.path.exists(pdf_file):\n",
        "        print(\"âŒ File not found.\")\n",
        "        return\n",
        "\n",
        "    file_size = os.path.getsize(pdf_file) / (1024 * 1024)\n",
        "    print(f\"ðŸ“Š File size: {file_size:.2f} MB\")\n",
        "\n",
        "    # Create splitter and process\n",
        "    splitter = CompletePDFSplitter()\n",
        "    splitter.debug_mode = False  # Reduce output for quick mode\n",
        "\n",
        "    # Generate output\n",
        "    timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "    output_file = f\"QUICK_SPLIT_{Path(pdf_file).stem}_{timestamp}.pdf\"\n",
        "\n",
        "    print(f\"ðŸš€ Quick processing...\")\n",
        "\n",
        "    # Use multi-method for best results\n",
        "    success = splitter.split_pdf_with_analysis(pdf_file, output_file, \"multi_method\")\n",
        "\n",
        "    if success:\n",
        "        print(f\"âœ… Done! Downloading {output_file}...\")\n",
        "\n",
        "        try:\n",
        "            files.download(output_file)\n",
        "            print(\"ðŸŽ‰ Success! Your split PDF has been downloaded.\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Download error: {e}\")\n",
        "            print(f\"ðŸ’¡ File saved as: {output_file}\")\n",
        "    else:\n",
        "        print(\"âŒ Processing failed.\")\n",
        "\n",
        "def batch_process_pdfs():\n",
        "    \"\"\"Process multiple PDFs if available\"\"\"\n",
        "\n",
        "    print(\"ðŸ“¦ BATCH PDF PROCESSING\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"ðŸ• Time: 2025-08-08 12:22:52 UTC\")\n",
        "    print(f\"ðŸ‘¤ User: Ravi-katta-dev\")\n",
        "    print(\"ðŸ“¦ Process multiple PDFs automatically\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Find all PDFs\n",
        "    all_pdfs = [f for f in os.listdir('.') if f.lower().endswith('.pdf')]\n",
        "    input_pdfs = [f for f in all_pdfs if not f.startswith(('SMART_', 'LINE_', 'PRECISION_', 'INTELLIGENT_', 'COMPREHENSIVE_', 'CUSTOM_', 'QUICK_'))]\n",
        "\n",
        "    if not input_pdfs:\n",
        "        print(\"âŒ No input PDF files found.\")\n",
        "        return\n",
        "\n",
        "    if len(input_pdfs) == 1:\n",
        "        print(f\"ðŸ“„ Only one PDF found: {input_pdfs[0]}\")\n",
        "        print(\"ðŸ’¡ Use quick_split_now() for single file processing\")\n",
        "        return\n",
        "\n",
        "    print(f\"ðŸ“š Found {len(input_pdfs)} PDFs to process:\")\n",
        "    for i, pdf in enumerate(input_pdfs, 1):\n",
        "        file_size = os.path.getsize(pdf) / (1024 * 1024)\n",
        "        print(f\"   {i}. {pdf} ({file_size:.2f} MB)\")\n",
        "\n",
        "    # Create splitter\n",
        "    splitter = CompletePDFSplitter()\n",
        "\n",
        "    # Process each PDF\n",
        "    successful_files = []\n",
        "    failed_files = []\n",
        "\n",
        "    for i, pdf_file in enumerate(input_pdfs, 1):\n",
        "        print(f\"\\nðŸ”„ Processing {i}/{len(input_pdfs)}: {pdf_file}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Generate output name\n",
        "        timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "        output_file = f\"BATCH_SPLIT_{i}_{Path(pdf_file).stem}_{timestamp}.pdf\"\n",
        "\n",
        "        try:\n",
        "            success = splitter.split_pdf_with_analysis(pdf_file, output_file, \"multi_method\")\n",
        "\n",
        "            if success:\n",
        "                successful_files.append(output_file)\n",
        "                print(f\"âœ… {pdf_file} processed successfully\")\n",
        "            else:\n",
        "                failed_files.append(pdf_file)\n",
        "                print(f\"âŒ {pdf_file} processing failed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_files.append(pdf_file)\n",
        "            print(f\"âŒ {pdf_file} error: {e}\")\n",
        "\n",
        "    # Results summary\n",
        "    print(f\"\\nðŸ“Š BATCH PROCESSING SUMMARY\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"âœ… Successful: {len(successful_files)}/{len(input_pdfs)}\")\n",
        "    print(f\"âŒ Failed: {len(failed_files)}/{len(input_pdfs)}\")\n",
        "\n",
        "    if successful_files:\n",
        "        print(f\"\\nðŸ“¥ Downloading successful files...\")\n",
        "\n",
        "        for output_file in successful_files:\n",
        "            try:\n",
        "                files.download(output_file)\n",
        "                print(f\"âœ… Downloaded: {output_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Download failed: {output_file} - {e}\")\n",
        "\n",
        "    if failed_files:\n",
        "        print(f\"\\nâŒ Failed files:\")\n",
        "        for failed_file in failed_files:\n",
        "            print(f\"   â€¢ {failed_file}\")\n",
        "\n",
        "def diagnose_pdf_issues():\n",
        "    \"\"\"Diagnose potential issues with PDF processing\"\"\"\n",
        "\n",
        "    print(\"ðŸ” PDF DIAGNOSTIC TOOL\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"ðŸ• Time: 2025-08-08 12:22:52 UTC\")\n",
        "    print(f\"ðŸ‘¤ User: Ravi-katta-dev\")\n",
        "    print(\"ðŸ” Comprehensive PDF analysis and issue detection\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Find PDFs\n",
        "    pdf_files = [f for f in os.listdir('.') if f.lower().endswith('.pdf') and not f.startswith(('SMART_', 'LINE_', 'PRECISION_', 'INTELLIGENT_'))]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"âŒ No PDF files found for diagnosis.\")\n",
        "        return\n",
        "\n",
        "    splitter = CompletePDFSplitter()\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"\\nðŸ” DIAGNOSING: {pdf_file}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Basic file info\n",
        "        try:\n",
        "            file_size = os.path.getsize(pdf_file) / (1024 * 1024)\n",
        "            print(f\"ðŸ“ File size: {file_size:.2f} MB\")\n",
        "\n",
        "            # Detailed validation\n",
        "            is_valid, message, pdf_info = splitter.validate_pdf_file(pdf_file)\n",
        "            print(f\"ðŸ“‹ Validation: {message}\")\n",
        "\n",
        "            if not is_valid:\n",
        "                continue\n",
        "\n",
        "            # Open PDF for detailed analysis\n",
        "            doc = fitz.open(pdf_file)\n",
        "\n",
        "            try:\n",
        "                # Page analysis\n",
        "                print(f\"\\nðŸ“„ Page Analysis:\")\n",
        "                print(f\"   â€¢ Total pages: {len(doc)}\")\n",
        "\n",
        "                if len(doc) > 0:\n",
        "                    first_page = doc[0]\n",
        "                    print(f\"   â€¢ Page size: {first_page.rect.width:.0f} x {first_page.rect.height:.0f}\")\n",
        "                    print(f\"   â€¢ Aspect ratio: {first_page.rect.width/first_page.rect.height:.2f}\")\n",
        "\n",
        "                    # Text analysis\n",
        "                    text_content = first_page.get_text()\n",
        "                    print(f\"   â€¢ Text length (first page): {len(text_content)} characters\")\n",
        "\n",
        "                    if len(text_content) < 50:\n",
        "                        print(\"   âš ï¸ Warning: Very little text detected\")\n",
        "\n",
        "                    # Check for common keywords\n",
        "                    keywords = ['question', 'answer', 'telegram', 'click', 'practice', 'exam']\n",
        "                    found_keywords = [kw for kw in keywords if kw.lower() in text_content.lower()]\n",
        "\n",
        "                    if found_keywords:\n",
        "                        print(f\"   â€¢ Content type indicators: {', '.join(found_keywords)}\")\n",
        "\n",
        "                # Quick line detection test\n",
        "                print(f\"\\nðŸ“ Line Detection Test:\")\n",
        "                line_test = splitter.detect_vertical_lines(pdf_file, sample_pages=3)\n",
        "\n",
        "                print(f\"   â€¢ Lines detected: {len(line_test.get('detected_lines', []))}\")\n",
        "                print(f\"   â€¢ Suggested split: {line_test.get('optimal_split', 0.5):.1%}\")\n",
        "                print(f\"   â€¢ Confidence: {line_test.get('confidence', 0):.1%}\")\n",
        "\n",
        "                if line_test.get('confidence', 0) > 0.7:\n",
        "                    print(\"   âœ… Excellent line detection\")\n",
        "                elif line_test.get('confidence', 0) > 0.4:\n",
        "                    print(\"   âš ï¸ Moderate line detection\")\n",
        "                else:\n",
        "                    print(\"   âŒ Poor line detection - may need manual ratio\")\n",
        "\n",
        "                # Content structure analysis\n",
        "                print(f\"\\nðŸ§  Content Structure:\")\n",
        "\n",
        "                blocks = first_page.get_text(\"dict\").get(\"blocks\", [])\n",
        "                text_blocks = [b for b in blocks if \"lines\" in b]\n",
        "\n",
        "                print(f\"   â€¢ Text blocks: {len(text_blocks)}\")\n",
        "\n",
        "                if text_blocks:\n",
        "                    x_positions = []\n",
        "                    for block in text_blocks:\n",
        "                        bbox = block[\"bbox\"]\n",
        "                        x_positions.extend([bbox[0], bbox[2]])\n",
        "\n",
        "                    x_positions.sort()\n",
        "                    page_width = first_page.rect.width\n",
        "\n",
        "                    # Check distribution\n",
        "                    left_content = sum(1 for x in x_positions if x < page_width * 0.5)\n",
        "                    right_content = sum(1 for x in x_positions if x >= page_width * 0.5)\n",
        "\n",
        "                    print(f\"   â€¢ Content distribution: {left_content} left, {right_content} right\")\n",
        "\n",
        "                    if abs(left_content - right_content) > 5:\n",
        "                        print(\"   âœ… Uneven distribution - good for splitting\")\n",
        "                    else:\n",
        "                        print(\"   âš ï¸ Even distribution - splitting may not be beneficial\")\n",
        "\n",
        "                # Recommendations\n",
        "                print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
        "\n",
        "                confidence = line_test.get('confidence', 0)\n",
        "                split_ratio = line_test.get('optimal_split', 0.5)\n",
        "\n",
        "                if confidence > 0.7:\n",
        "                    print(f\"   ðŸŸ¢ PROCEED: Use automatic detection ({split_ratio:.1%})\")\n",
        "                elif confidence > 0.4:\n",
        "                    print(f\"   ðŸŸ¡ CAUTION: Review suggested split ({split_ratio:.1%})\")\n",
        "                else:\n",
        "                    print(f\"   ðŸ”´ MANUAL: Consider manual inspection\")\n",
        "\n",
        "                # Check for potential issues\n",
        "                issues = []\n",
        "\n",
        "                if file_size > 100:\n",
        "                    issues.append(\"Very large file - processing may be slow\")\n",
        "\n",
        "                if len(doc) > 200:\n",
        "                    issues.append(\"Many pages - consider batch processing\")\n",
        "\n",
        "                if split_ratio < 0.2 or split_ratio > 0.8:\n",
        "                    issues.append(f\"Unusual split ratio ({split_ratio:.1%})\")\n",
        "\n",
        "                if len(text_content) < 100:\n",
        "                    issues.append(\"Very little text - might be image-based PDF\")\n",
        "\n",
        "                if issues:\n",
        "                    print(f\"\\nâš ï¸ POTENTIAL ISSUES:\")\n",
        "                    for issue in issues:\n",
        "                        print(f\"   â€¢ {issue}\")\n",
        "                else:\n",
        "                    print(f\"\\nâœ… No issues detected - PDF looks good for processing!\")\n",
        "\n",
        "            finally:\n",
        "                doc.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Diagnostic error: {e}\")\n",
        "\n",
        "def show_help_and_usage():\n",
        "    \"\"\"Show comprehensive help and usage guide\"\"\"\n",
        "\n",
        "    print(\"ðŸ“š COMPLETE PDF SPLITTER - HELP & USAGE GUIDE\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"ðŸ• Time: 2025-08-08 12:22:52 UTC\")\n",
        "    print(f\"ðŸ‘¤ User: Ravi-katta-dev\")\n",
        "    print(f\"ðŸ“± Session: {datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(\"\"\"\n",
        "ðŸŽ¯ AVAILABLE FUNCTIONS:\n",
        "\n",
        "1. ðŸš€ QUICK PROCESSING:\n",
        "   quick_split_now()                    - Fast split with smart defaults\n",
        "\n",
        "2. ðŸŽ® INTERACTIVE MODE:\n",
        "   run_interactive_mode()               - Step-by-step guided processing\n",
        "\n",
        "3. ðŸ§  COMPREHENSIVE ANALYSIS:\n",
        "   test_comprehensive_analysis()        - Test all detection methods\n",
        "   run_intelligent_split()              - Full multi-method processing\n",
        "\n",
        "4. ðŸ“¦ BATCH PROCESSING:\n",
        "   batch_process_pdfs()                 - Process multiple PDFs\n",
        "\n",
        "5. ðŸ” DIAGNOSTIC TOOLS:\n",
        "   diagnose_pdf_issues()                - Analyze PDF structure and issues\n",
        "\n",
        "6. â„¹ï¸ HELP & INFO:\n",
        "   show_help_and_usage()                - This help guide\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "ðŸŽ¯ RECOMMENDED WORKFLOW:\n",
        "\n",
        "For First-Time Users:\n",
        "1. Upload your PDF file\n",
        "2. Run: diagnose_pdf_issues()          # Check for any issues\n",
        "3. Run: test_comprehensive_analysis()   # See what methods work best\n",
        "4. Run: run_interactive_mode()         # Process with guidance\n",
        "\n",
        "For Quick Processing:\n",
        "1. Upload your PDF file\n",
        "2. Run: quick_split_now()              # Fast processing\n",
        "\n",
        "For Multiple Files:\n",
        "1. Upload multiple PDF files\n",
        "2. Run: batch_process_pdfs()           # Process all at once\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "ðŸ”§ DETECTION METHODS:\n",
        "\n",
        "ðŸ“ Line Detection:\n",
        "   - Finds actual vertical lines in your PDF\n",
        "   - Perfect for documents with visible dividers\n",
        "   - High accuracy for structured documents\n",
        "\n",
        "ðŸ§  Content Analysis:\n",
        "   - Analyzes text density and distribution\n",
        "   - Good for documents without visible lines\n",
        "   - Works with natural content boundaries\n",
        "\n",
        "ðŸ‘ï¸ Visual Pattern Recognition:\n",
        "   - Recognizes visual layout patterns\n",
        "   - Detects column structures and alignments\n",
        "   - Useful for complex layouts\n",
        "\n",
        "ðŸ“‹ Document Structure Analysis:\n",
        "   - Understands document type and purpose\n",
        "   - Optimizes for exam papers, forms, etc.\n",
        "   - Context-aware processing\n",
        "\n",
        "ðŸŽ¯ Multi-Method Fusion:\n",
        "   - Combines all methods for best accuracy\n",
        "   - Highest confidence results\n",
        "   - Recommended for unknown document types\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "ðŸ’¡ TROUBLESHOOTING:\n",
        "\n",
        "âŒ \"No PDF files found\":\n",
        "   - Upload a PDF file first\n",
        "   - Check file has .pdf extension\n",
        "   - Restart runtime if needed\n",
        "\n",
        "âŒ \"Processing failed\":\n",
        "   - Run diagnose_pdf_issues() to check PDF\n",
        "   - Try different detection method\n",
        "   - Check if PDF is password protected\n",
        "\n",
        "âŒ \"Download failed\":\n",
        "   - File is still processed locally\n",
        "   - Try downloading manually\n",
        "   - Check internet connection\n",
        "\n",
        "âŒ \"Low confidence detection\":\n",
        "   - PDF might have unusual layout\n",
        "   - Try manual ratio adjustment\n",
        "   - Use visual inspection\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "ðŸŽ“ TIPS FOR BEST RESULTS:\n",
        "\n",
        "âœ… PDF Quality:\n",
        "   - Use high-quality, text-based PDFs\n",
        "   - Avoid heavily compressed files\n",
        "   - Ensure text is selectable\n",
        "\n",
        "âœ… Document Type:\n",
        "   - Works best with structured documents\n",
        "   - Exam papers, forms, and reports ideal\n",
        "   - Two-column layouts process excellently\n",
        "\n",
        "âœ… File Size:\n",
        "   - Files under 50MB process fastest\n",
        "   - Large files may take longer\n",
        "   - Consider batch processing for multiple files\n",
        "\n",
        "âœ… Split Ratios:\n",
        "   - 40-60% typically work best\n",
        "   - Extreme ratios (20% or 80%) may indicate issues\n",
        "   - Trust high-confidence detections\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "ðŸ†˜ SUPPORT:\n",
        "\n",
        "If you encounter issues:\n",
        "1. Run diagnose_pdf_issues() first\n",
        "2. Check the troubleshooting section above\n",
        "3. Try different processing methods\n",
        "4. Consider manual ratio adjustment\n",
        "\n",
        "For best results with your exam papers:\n",
        "- Use quick_split_now() for simple cases\n",
        "- Use run_interactive_mode() for guidance\n",
        "- Use test_comprehensive_analysis() to verify detection\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")\n",
        "\n",
        "# =============================================================================\n",
        "# ðŸš€ SYSTEM INITIALIZATION AND STATUS\n",
        "# =============================================================================\n",
        "\n",
        "def system_status():\n",
        "    \"\"\"Show current system status and available files\"\"\"\n",
        "\n",
        "    print(\"ðŸ“Š SYSTEM STATUS REPORT\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"ðŸ• Current Time: 2025-08-08 12:22:52 UTC\")\n",
        "    print(f\"ðŸ‘¤ Current User: Ravi-katta-dev\")\n",
        "    print(f\"ðŸ Python Environment: Google Colab\")\n",
        "    print(f\"ðŸ“¦ PyMuPDF Version: {fitz.version[0] if hasattr(fitz, 'version') else 'Unknown'}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Check current directory contents\n",
        "    all_files = os.listdir('.')\n",
        "    pdf_files = [f for f in all_files if f.lower().endswith('.pdf')]\n",
        "\n",
        "    print(f\"ðŸ“ Current Directory Analysis:\")\n",
        "    print(f\"   â€¢ Total files: {len(all_files)}\")\n",
        "    print(f\"   â€¢ PDF files: {len(pdf_files)}\")\n",
        "\n",
        "    if pdf_files:\n",
        "        print(f\"\\nðŸ“‹ PDF Files Found:\")\n",
        "\n",
        "        input_pdfs = []\n",
        "        output_pdfs = []\n",
        "\n",
        "        for pdf in pdf_files:\n",
        "            file_size = os.path.getsize(pdf) / (1024 * 1024)\n",
        "\n",
        "            if pdf.startswith(('SMART_', 'LINE_', 'PRECISION_', 'INTELLIGENT_', 'COMPREHENSIVE_', 'CUSTOM_', 'QUICK_', 'BATCH_')):\n",
        "                output_pdfs.append((pdf, file_size))\n",
        "            else:\n",
        "                input_pdfs.append((pdf, file_size))\n",
        "\n",
        "        if input_pdfs:\n",
        "            print(f\"\\nðŸ“¥ Input PDFs ({len(input_pdfs)}):\")\n",
        "            for pdf, size in input_pdfs:\n",
        "                print(f\"   ðŸ“„ {pdf} ({size:.2f} MB)\")\n",
        "\n",
        "        if output_pdfs:\n",
        "            print(f\"\\nðŸ“¤ Processed PDFs ({len(output_pdfs)}):\")\n",
        "            for pdf, size in output_pdfs:\n",
        "                print(f\"   ðŸ“„ {pdf} ({size:.2f} MB)\")\n",
        "    else:\n",
        "        print(\"\\nâŒ No PDF files found\")\n",
        "        print(\"ðŸ’¡ Upload a PDF file to get started\")\n",
        "\n",
        "    # Memory status\n",
        "    print(f\"\\nðŸ’¾ Memory Status:\")\n",
        "    try:\n",
        "        import psutil\n",
        "        memory = psutil.virtual_memory()\n",
        "        print(f\"   â€¢ Available: {memory.available / (1024**3):.1f} GB\")\n",
        "        print(f\"   â€¢ Usage: {memory.percent:.1f}%\")\n",
        "    except:\n",
        "        print(\"   â€¢ Memory info not available\")\n",
        "\n",
        "    # Recommendations\n",
        "    print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"   ðŸ”¸ Upload a PDF file first\")\n",
        "        print(\"   ðŸ”¸ Run show_help_and_usage() for guidance\")\n",
        "    elif input_pdfs and not output_pdfs:\n",
        "        print(\"   ðŸ”¸ Ready to process! Try quick_split_now()\")\n",
        "        print(\"   ðŸ”¸ Or run run_interactive_mode() for guided processing\")\n",
        "    elif input_pdfs and output_pdfs:\n",
        "        print(\"   ðŸ”¸ Some files already processed\")\n",
        "        print(\"   ðŸ”¸ Check output files or process remaining inputs\")\n",
        "    else:\n",
        "        print(\"   ðŸ”¸ Only output files found\")\n",
        "        print(\"   ðŸ”¸ Upload new input PDFs or download existing outputs\")\n",
        "\n",
        "# Initialize the complete system\n",
        "print(\"ðŸŽ¯ COMPLETE PDF SPLITTER v4.0 - FULLY LOADED!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"ðŸ• System Time: 2025-08-08 12:22:52 UTC\")\n",
        "print(f\"ðŸ‘¤ Current User: Ravi-katta-dev\")\n",
        "print(f\"ðŸš€ Status: All systems operational\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Show system status\n",
        "system_status()\n",
        "\n",
        "print(f\"\\nðŸŽ® QUICK START COMMANDS:\")\n",
        "print(\"-\" * 30)\n",
        "print(\"ðŸš€ quick_split_now()              # Fast processing\")\n",
        "print(\"ðŸŽ® run_interactive_mode()         # Step-by-step guidance\")\n",
        "print(\"ðŸ§ª test_comprehensive_analysis()  # Test all methods\")\n",
        "print(\"ðŸ” diagnose_pdf_issues()          # Check PDF health\")\n",
        "print(\"ðŸ“š show_help_and_usage()          # Complete guide\")\n",
        "print(\"ðŸ“Š system_status()                # System information\")\n",
        "\n",
        "print(f\"\\nâœ¨ Ready for processing! Choose a command above to get started.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ðŸŽ¯ COMPLETE INTELLIGENT PDF SPLITTER - Full Featured Solution\n",
        "# =============================================================================\n",
        "# Author: Advanced PDF Processing System\n",
        "# Date: 2025-08-08 12:18:45 UTC\n",
        "# User: Ravi-katta-dev\n",
        "# Version: 4.0 - Complete Solution\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "import zipfile\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, List, Dict, Any\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import fitz  # PyMuPDF\n",
        "    from tqdm.auto import tqdm\n",
        "    from google.colab import files\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    print(\"âœ… All imports successful!\")\n",
        "    print(f\"ðŸ• Session started: 2025-08-08 12:18:45 UTC\")\n",
        "    print(f\"ðŸ‘¤ User: Ravi-katta-dev\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Import error: {e}\")\n",
        "    print(\"Please install required packages:\")\n",
        "    print(\"!pip install PyMuPDF tqdm ipywidgets\")\n",
        "    raise\n",
        "\n",
        "# [all class and all function definitions remain unchanged from your original code]\n",
        "# Only the error about indentation is fixed - all top-level functions, including\n",
        "# run_interactive_mode, are at the top level, not nested inside other functions.\n",
        "\n",
        "# ... [CompletePDFSplitter class definition from your original file here, unchanged] ...\n",
        "\n",
        "class CompletePDFSplitter:\n",
        "    \"\"\"Complete PDF Splitter with all advanced features\"\"\"\n",
        "    # ... (all methods unchanged from your original file) ...\n",
        "    # The class body is omitted here for brevity, but you should include\n",
        "    # the entire class exactly as in your original code.\n",
        "\n",
        "# =============================================================================\n",
        "# ðŸš€ MAIN EXECUTION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def test_comprehensive_analysis():\n",
        "    # ... (function body unchanged) ...\n",
        "    pass  # Replace with your original function code\n",
        "\n",
        "def run_intelligent_split():\n",
        "    \"\"\"Run intelligent PDF splitting with comprehensive analysis\"\"\"\n",
        "    # ... (function body unchanged) ...\n",
        "    pass  # Replace with your original function code\n",
        "\n",
        "def run_interactive_mode():\n",
        "    \"\"\"Run interactive mode with step-by-step guidance\"\"\"\n",
        "    # ... (function body unchanged) ...\n",
        "    pass  # Replace with your original function code\n",
        "\n",
        "def quick_split_now():\n",
        "    # ... (function body unchanged) ...\n",
        "    pass  # Replace with your original function code\n",
        "\n",
        "def batch_process_pdfs():\n",
        "    # ... (function body unchanged) ...\n",
        "    pass  # Replace with your original function code\n",
        "\n",
        "def diagnose_pdf_issues():\n",
        "    # ... (function body unchanged) ...\n",
        "    pass  # Replace with your original function code\n",
        "\n",
        "def show_help_and_usage():\n",
        "    # ... (function body unchanged) ...\n",
        "    pass  # Replace with your original function code\n",
        "\n",
        "def system_status():\n",
        "    # ... (function body unchanged) ...\n",
        "    pass  # Replace with your original function code\n",
        "\n",
        "# Initialize the complete system\n",
        "print(\"ðŸŽ¯ COMPLETE PDF SPLITTER v4.0 - FULLY LOADED!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"ðŸ• System Time: 2025-08-08 12:22:52 UTC\")\n",
        "print(f\"ðŸ‘¤ Current User: Ravi-katta-dev\")\n",
        "print(f\"ðŸš€ Status: All systems operational\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Show system status\n",
        "system_status()\n",
        "\n",
        "print(f\"\\nðŸŽ® QUICK START COMMANDS:\")\n",
        "print(\"-\" * 30)\n",
        "print(\"ðŸš€ quick_split_now()              # Fast processing\")\n",
        "print(\"ðŸŽ® run_interactive_mode()         # Step-by-step guidance\")\n",
        "print(\"ðŸ§ª test_comprehensive_analysis()  # Test all methods\")\n",
        "print(\"ðŸ” diagnose_pdf_issues()          # Check PDF health\")\n",
        "print(\"ðŸ“š show_help_and_usage()          # Complete guide\")\n",
        "print(\"ðŸ“Š system_status()                # System information\")\n",
        "\n",
        "print(f\"\\nâœ¨ Ready for processing! Choose a command above to get started.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "rqW9tVxU0JCC",
        "outputId": "bf861291-ff22-4388-c13c-db94a9d94eb3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âŒ Import error: No module named 'fitz'\n",
            "Please install required packages:\n",
            "!pip install PyMuPDF tqdm ipywidgets\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'fitz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-49194434.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mfitz\u001b[0m  \u001b[0;31m# PyMuPDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fitz'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}